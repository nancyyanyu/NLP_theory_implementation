# What are Vectors



We are always interested in a matrix of size **V x D**

- $V$ = vocabulary size(# of total words
- $D$ = vector dimensionality
  - Ex. if we're counting up how many times a word appears in a set of books, $D$ = # of books

**Word Embedding**: a vector that represents a word



# What is a Word Analogy

> Women - Men $\approx$ Queen - King

<img src="./1.Working with WordVectors1.png" style="zoom:50%;" />

## How to find analogy ?

SomeVector = King - Man + Women

Key point: Use vector distance to find the closest matching word



### Distance

**Euclidean dist**: $|a-b|^2=(a_1-b_1)^2+(a_2-b_2)^2+...+(a_D-b_D)^2$

**Cosine dist**: $1-\frac{a^Tb}{|a||b|}$, since $a^Tb=|a||b|\cos\theta$



## Why is this cool?

- If we have $V$ words, and each is represented by a vector of size $D$, Then we have a $V \times D$ matric.

- Algos like **word2vec** or **GloVe** have no concept of analogies.
  -  So the fact that the word analogies suddenly emerge out of the training process is a very intriguing result.

- TF-IDF won't give us word analogies



# Find and assess word vectors using TF-IDF and t-SNE

## Dimensionality

-  **TF-IDF** and counting across many words and documents results in large dimentionality
- **t-SNE** help us *reduce* dimensionality
- Any **word embedding matrix** is going to be a table of shape $V\times D$
- Do the word vectors from TF-IDF  make sense?

## Similarity

- We know that TF-IDF won't give us good analogies
- But that shouldn't stop us from trying
- Neighbors should still be similar(related) words



# Pretrained word vectors from word2vec





# Pretrained word vectors from GloVe





# Text Classification with word vectors

## Text Classification

- Data representation: **bag-of-words**
- Don't consider the order of the words in a sentence. Ex. "Toy dog" is treated the same as "dog toy"
- Both word counting and TF-IDF are examples



## Bag-of-words

- How can we go from word2vec/GloVe vectors to bag-of-words
- Ex.
  - "I like eggs"
  - Feature vector = $[vec("I")+vec("like")+vec("eggs")]/3$

$$
feature=\frac{1}{|sentence|} \sum_{w\in sentence}vec(w)
$$

- Each sample is now represented by a **feature vector**
- Use any classifier!
  - Naive Bayes
  - Logistic Regression
  - Random Forest
  - Extra Trees



## What's the "meaning" of this?

- **Feature vectors** must be "good" in order to get acceptable classifcation accuracy 
- This is in fact a method of **evaluating**. how good a word embedding is

- Other methods of evaluating: word analogies, similarity
- Ultimately these are just a proxy for how well they will work on your particular dataset for your paticular problem

