# Matrix Factorization for Recommender Systems

## **The User-Movie ratings matrix**:

-  N users
- M movies
- Shape (R) = N x M

<img src="4.Word Embeddings and GloVe1.png" style="zoom:30%;" />

### Most values are missing!

- Most users have not rated most movies

- It is "sparse"

  - <u>Not</u> sparse meaning lots of zeros

  

  



### Sparsity is good!

In reality having missing values is a good thing. It helps to remember what the application is recommender systems.

- Recommender system: tell you about a movie you might like that you haven't seen yet
- A ratings matrix with no missing values means that every user has seen and rated every single movie.
  - nothing to recommend



## Collarborative Filtering

- Out of all the users in our database there are going to be other users that are similar to you but not exactly the same as you
- What that means is you have some movies in common that you both liked.
- It's **collaborative** because we use other users' data to help make predictions for you



## Key Property: Redundancy

Redundancy: two features being very correlated with one or the other. If they are correlated with each other, we can predict one from the other

- Documents are redundant
  - Do I   need 1 million documents to tell me "gravity" and "quantum" are related?
- Movie ratings can be redundant
  - If everyone who likes Star Wars Episode 1 also like Star Wars Episode 2 then I don't need to know the rating for both movies.

### How do we exploit redundancy

- **Dimensionality reduction**: We want to reduce the dimensitionality of the data 
- One method: **Singular Value Decomposition (SVD)**
  - decompose some matrix $X$ into three separate matrices.

$$
X=USV^T
$$

##### How is it "reduced"?

- $U,S,V$ together take up more space than $X$!

$$
X_{N \times D}=U_{N \times D}S_{D \times D}V^T{D \times D}
$$

##### We can "chop off" parts of $U,S,V$

- Choose $K<<D$
- Eqaulity is now only approximate

<img src="4.Word Embeddings and GloVe2.png" style="zoom:30%;" />

#### Quick calculations

- $N=1$ Million, $D=500000$, $K=10$
- $size(X)=N\times D=10^6\times 5 \cdot10^5=5 \times 10^{11}$
- $size(U)=N \times K=10 million$
- $size(S)=10 \times 10=100$
- $size(V)=500000 \times 10=5million$
- 15 million / 500 billion $=3 \times 10^{-5}$





## You don't have to know about SVD!

- SVDdoesn't actually work for a recommended system.
  - SVD requires every value to be filled in (no missing value)
  -  SVD might not scale if we have a large number of users and a large number of items.



## Matrix Factorization

- SVD lets us know it's possible to factorize a matrix
- We want to factorize R into just 2 matrices

$$
R \approx \hat{R}=WU^T
$$

### What are the savings?

- $N=100000, M=20000$
- $size(R)=10^5 \times 2\times 10^4=2 billion$
- 20 million ratings
- 20 million / 2 billion =0.01
  - 99% empty
- $K=10$
- $size(W)=N \times K=1 million$
- $size(U)=M\times K=200000$
- 1.2 million parameters / 20 million ratings = 6%



### How do we use the model?

- Our prediction matrix has no mission values
  $$
  R \approx \hat{R}=WU^T
  $$

- Each individual prediction is just a simple **dot product** between 2 $K$-length vectors
  $$
  \hat{r}_{ij}=W_iU_j^T
  $$

- Recommendations would be movies we haven't seen yet, sorted by their rating predictions



### Why does it make sense?

- **Dot product** is just cosine similarity

- We can pretend each of the $K$ latent dimentions is a meaningful interpretable feature
  $$
  sim(A,B) = \cos{\theta}=\frac{A \cdot B}{||A||||B||}
  $$
  

## Evaluating the model

- We can only evaluate the accuracy of our model on ratings that are known.
- Regression since we want to predict ratings
- We want the squared error only for ratings that are known

$$
J=\sum_{i,j \in \mho}(r_{ij}-\hat{r}_{ij})^2 \\
\mho = \text{set of all pairs (i,j) where user i rated movie j}
$$

- Analogious to SVD cost
  $$
  J=\sum_{i=1}^N \sum_{j=1}^D (X_{ij}-\hat{X}_{ij})^2
  $$
  



## Summary

- Ratings matrix
  $$
  r_{ij} = \text{what user i rated movie j} \\
  $$

- Model
  $$
  \hat{r}_{ij}=W_iU_j^T
  $$
  

- Cost
  $$
  J=\sum_{i,j \in \mho}(r_{ij}-\hat{r}_{ij})^2 \\
  $$

- Useful sets

$$
\mho_j = \text{set of users i who rated movie j} \\
\Psi_i = \text{set of movies j which user i rated} \\
$$

# Training

**Squared error loss**
$$
J=\sum_{i,j \in \mho}(r_{ij}-\hat{r}_{ij})^2 =\sum_{i,j \in \mho} (r_{ij}-w_i^T u_j )^2 \\
\mho = \text{set of all pairs (i,j) where user i rated movie j}
$$

## Minimize the loss

### Solving for W

- Careful about which sets are being summed over 
  - For $j$, we want to sum over all ratings
  - For a particular user vector $w_i$, we only care about movies that user rated (because only those ratings involve $w_i$)

$$
\frac{\partial J}{\partial w_i} =2\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j )(-u_j)=0
$$

- Try to isolate $w_i$

  - It's stuck inside a dot product!

  $$
  \sum_{j \in \Psi_i} (w_i^Tu_j)u_j=\sum_{j \in \Psi_i}r_{ij}u_j \\
  \text{scalar $\times$ vector} = \text{ vector $\times$ scalar } \\
  
  \sum_{j \in \Psi_i} u_j (u_j ^T w_i)= \sum_{j \in \Psi_i} r_{ij}u_j
  $$

   

- Drop the brackets

$$
\sum_{j \in \Psi_i} u_j u_j ^T w_i= \sum_{j \in \Psi_i} r_{ij}u_j 
$$

- Summartion doesn't actually depend on $i$

$$
(\sum_{j \in \Psi_i} u_j u_j ^T )w_i= \sum_{j \in \Psi_i} r_{ij}u_j  \\
w_i=\frac{\sum_{j \in \Psi_i} r_{ij}u_j}{\sum_{j \in \Psi_i} u_j u_j ^T }
$$

- `x=np.linalg.solve(A,b)` for solving  $Ax=b$



### Solving for U

- Loss is "symmetric" in W and U, so the steps should be same
- Be CAREFUL which set to sum over

$$
\frac{\partial J}{\partial u_j} =2\sum_{i \in \mho_j} (r_{ij}-w_i^T u_j )(-w_i)=0
$$

- Try to isolate $u_j$

$$
\sum_{i \in \mho_j} (w_i^Tu_j)w_i=\sum_{i \in \Psi_i}r_{ij}w_i \\
\sum_{i \in \mho_j} w_i w_i ^T u_j= \sum_{i \in \mho_j} r_{ij}w_i
$$

$$
u_j=\frac{\sum_{i \in \mho_j} r_{ij}w_i}{\sum_{i \in \mho_j} w_i w_i ^T }
$$

## 2-way dependency

- Solution for W depends on U
- Solution for U depends on W

We have two parameters W and you and therefore they both need to be updated.



## Training algorithm

- Simply apply the equations as it, iteratively
- Is called "**alternating least squares**"	

$$
\begin{align}
&\text{W=randn(N,K); U=randn(M,K);} \\
&\text{for t in range(T):}\\

&w_i=\frac{\sum_{j \in \Psi_i} r_{ij}u_j}{\sum_{j \in \Psi_i} u_j u_j ^T } \\


&u_j=\frac{\sum_{i \in \mho_j} r_{ij}w_i}{\sum_{i \in \mho_j} w_i w_i ^T }
\end{align}
$$

# Expanding Matrix Factorization Model

## Bias Terms

- Different people rate things differently.
- Add bias terms to the MF model

$$
\hat{r}_{ij}=w_i^Tu_j+b_i+c_j+\mu \\
b_i=\text{user bias} \\
c_j=\text{movie bias} \\
\mu=\text{global average}
$$

## Training

- Start with objective

$$
J=\sum_{i,j \in \mho}(r_{ij}-\hat{r}_{ij})^2 \\
\hat{r}_{ij}=w_i^Tu_j+b_i+c_j+\mu \\
$$

#### Solving for W

$$
\frac{\partial J}{\partial w_i} =2\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -b_i-c_j-\mu)(-u_j)=0 \\
\sum_{j \in \Psi_i} (w_i^Tu_j)u_j=\sum_{j \in \Psi_i}(r_{ij} -b_i-c_j-\mu)u_j \\
w_i=\frac{\sum_{j \in \Psi_i} (r_{ij} -b_i-c_j-\mu)u_j}{\sum_{j \in \Psi_i} u_j u_j ^T } \\
$$

#### Solving for U

$$
u_j=\frac{\sum_{i \in \mho_j} (r_{ij} -b_i-c_j-\mu)w_i}{\sum_{i \in \mho_j} w_i w_i ^T }
$$

#### Solving for b

$$
\frac{\partial J}{\partial b_i} =2\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -b_i-c_j-\mu)(-1)=0 \\
$$

- Moving a variable outside a summation is equal to the variable itself.
- $b_i$ is the average deviation between target and modeling prediction, if the prediction did not involve $b_i$
- $b_i$ is exactly how much you need to add to the model prediction without $b_i$

$$
b_i =\frac{1}{ |\Psi_i|} \sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -c_j-\mu)
$$

#### Solving for c

$$
\frac{\partial J}{\partial c_j} =2\sum_{i \in  \mho_j} (r_{ij}-w_i^T u_j -b_i-c_j-\mu)(-1)=0 \\
c_j =\frac{1}{ |\mho_j|} \sum_{i \in \mho_j} (r_{ij}-w_i^T u_j -b_i-\mu)
$$

### Summary

$$
w_i=\frac{\sum_{j \in \Psi_i} (r_{ij} -b_i-c_j-\mu)u_j}{\sum_{j \in \Psi_i} u_j u_j ^T } \\
u_j=\frac{\sum_{i \in \mho_j} (r_{ij} -b_i-c_j-\mu)w_i}{\sum_{i \in \mho_j} w_i w_i ^T } \\
b_i =\frac{1}{ |\Psi_i|} \sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -c_j-\mu) \\
c_j =\frac{1}{ |\mho_j|} \sum_{i \in \mho_j} (r_{ij}-w_i^T u_j -b_i-\mu)
$$



# Regularization for Matrix Factorization

## Regularization

A technique that helps to prevent overfitting and help generalization.

- $||*||_F$ is **Euclidean norm** $\sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2}$

$$
J=\sum_{i,j \in \mho}(r_{ij}-\hat{r}_{ij})^2 +\lambda(||W||_F^2+||U||_F^2+||b||_2^2+||c||_2^2)
$$

### Solve for W

- Derivatives are additive, just need to differentiate the 2nd term and add it to the existing derivative

$$
\frac{\partial J}{\partial w_i} =2\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -b_i-c_j-\mu)(-u_j)+2\lambda w_i=0 \\
||W||_F^2 = \sum_{i=1}^N \sum_{k=1}^K |w_{ik}|^2=\sum_{i=1}^N||w_i||^2_2= \sum_{i=1}^N w_i^T w_i\\
w_i=\frac{\sum_{j \in \Psi_i} (r_{ij} -b_i-c_j-\mu)u_j}{\sum_{j \in \Psi_i} u_j u_j ^T +\lambda I} \\
$$

### Solve for U

$$
u_j=\frac{\sum_{i \in \mho_j} (r_{ij} -b_i-c_j-\mu)w_i}{\sum_{i \in \mho_j} w_i w_i ^T +\lambda I} \\
$$

### Solve for b

$$
\frac{\partial J}{\partial b_i} =2\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -b_i-c_j-\mu)(-1)+2\lambda b_i=0 \\
\sum_{j \in \Psi_i} b_i+\lambda b_i=\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -c_j-\mu)\\
 b_i(\sum_{j \in \Psi_i} 1+\lambda )=\sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -c_j-\mu)\\
b_i =\frac{1}{ |\Psi_i|+\lambda} \sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -c_j-\mu) \\
$$

### Solve for c

$$
c_j =\frac{1}{ |\mho_j|+\lambda} \sum_{i \in \mho_j} (r_{ij}-w_i^T u_j -b_i-\mu)
$$

## Summary

$$
w_i=\frac{\sum_{j \in \Psi_i} (r_{ij} -b_i-c_j-\mu)u_j}{\sum_{j \in \Psi_i} u_j u_j ^T +\lambda I} \\
u_j=\frac{\sum_{i \in \mho_j} (r_{ij} -b_i-c_j-\mu)w_i}{\sum_{i \in \mho_j} w_i w_i ^T +\lambda I} \\
b_i =\frac{1}{ |\Psi_i|+\lambda} \sum_{j \in \Psi_i} (r_{ij}-w_i^T u_j -c_j-\mu) \\
c_j =\frac{1}{ |\mho_j|+\lambda} \sum_{i \in \mho_j} (r_{ij}-w_i^T u_j -b_i-\mu)
$$





# GloVe - Global Vectors for Word Representation



**Context distance**

> "I love dogs and cats"

$X(I,love)+=1$

$X(I,dog)+=0.5$

$X(I,and)+=1/3$

- **co-occurrence matrix**: shape (V,V)

- Many zeros (sparse matrix)

  - Recommender systems also give us a sparse matrix, but GLoVe values are not missing

- Words often have *long tailed distribution*. -> non-zero values will be very large

  - So we will take the log - $\log{X(i,j)}$ will be the **target**
  - Add $1$ before taking the $log$ so we don't have NaNs

- Weight every $(i,j)$th entry by its x value.
  $$
  f(X)=(X/Xmax)^\alpha \text{  if } X< Xmax\text{ else }1 \\
  \alpha=0.75, Xmax=100
  $$

  - If it's $x$ value is sufficiently large  --- 1

  - If it's $x$ value is small --- give it a weight 
    $$
    J=\sum_i \sum_j f(X_{ij})(w_i^Tu_j-\log{X_{ij}})^2 \\
    i=1..N \\
    j=1...M
    $$
    <img src="4.Word Embeddings and GloVe3.png" style="zoom:60%;" />

- No missing value


$$
w_i=\frac{\sum_{j } f(X_{ij})(\log{X_{ij}} -b_i-c_j-\mu)u_j}{\sum_{j } f(X_{ij})u_ju_j^T} \\
u_j=\frac{\sum_{i }f(X_{ij}) (\log{X_{ij}} -b_i-c_j-\mu)w_i}{\sum_{i }f(X_{ij}) w_i w_i ^T } \\
b_i =\frac{1}{ \sum_j f(X_{ij})} \sum_{j } f(X_{ij})(\log{X_{ij}}-w_i^T u_j -c_j-\mu) \\
c_j =\frac{1}{ \sum_i f(X_{ij})} \sum_{i }  f(X_{ij})(\log{X_{ij}}-w_i^T u_j -b_i-\mu)
$$


## Training GLoVe

Ways to train:

1. Alternating least squares
2. Gradient descent
3. SVD







# Training GloVe with SVD

## In recommender systems

- Shape(W) = N x K
  - Users matrix (N users)
  - Each row is a user vector
- Shape(S) = K x K
  - Diagonal matrix
  - Variance of the K dimensions
  - More variance = more important,  this latent dimension gives more information
- Shape(U) = M x K
  - Movie matrix
  - Each row is a movie with K size latent  vector

<img src="4.Word Embeddings and GloVe4.png" style="zoom:50%;" />

## Structure is the same

- SVD

$$
X=WSU^T=(WS)U^T=W^{'}U^T
$$

- Matrix factorization

$$
R \approx \hat{R} = WU^T
$$



## Why SVG won't work in Recommender Systems



- SVD is **sparse** - most of the ratings are missing

  - SVD algorithm simply can't be used

- GloVe matrix is **sparse** - most values are $0$

  - 0 is meaningful value - 2 words never appeared in each other's context

- Typical implementation of SVD are not scalable

  - Will work since we limit the vocab size

  



## What about f(X)?

- Recall: $f(X)$ is a weighting that applied to the squared errer. SVD minimizes the unweighted squared error

$$
J=\sum_{i=1}^N \sum_{j=1}^D (X_{ij}-\hat{X}_{ij})^2
$$

- For matrix factorization, we'd like to weight each value in $X$ by $f()$
- Since we can't do this in scikit-learn, let's just forget about it and see what happens

$$
J=\sum_{i=1}^V \sum_{j=1}^V  f(X_{ij})(\log{X_{ij}}-\log{\hat{X}_{ij}})^2
$$

## In Code

- `TruncatedSVD`
  - `fit(X)`: Find W,S,U
  - `Z=transform(X)`
    - Linear transformation of X
    - `Z=XU`

Note: we don't care about $Z$, we want $W$. However, scikit-learn SVD doesn't save $W$  when fitting



### Re-finding W

$$
X=WSU^T \\
Z=XU \\
Z=(WSU^T)U \\
Z=WS \text{ since } U^TU=I \\
W=ZS^{-1}
$$

### Scikit-learn

- Read documentation to know which math symbols corrspond to which variable names
  - $U$ is referred to as `model.components_`
  - $S$ is referred to as `model.explained_variance` (diagonal elements only)

