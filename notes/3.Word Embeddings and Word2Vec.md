# CBOW - continuous bag of words

<img src="3.Word Embeddings and Word2Vec1.png" style="zoom:50%;" />

> "The quick brown fox jumps over the lazy dog."

- Context size could be considered 5-10
- The input weight is $W^{(1)}$ for all input words (same weight used multiple times)



## A simple article spinner

"I <u>walked</u> to the store"

"I <u>went</u> to the store"

Idea: find the probabilities
$$
p(w_t|w_{t-1},w_{t+1})
$$
And sample from them to get words that can replace the original. This is just CBOW with a context size of 1!



## The mechanics of CBOW

Step 1: Find the mean of input word embedding of the context words
$$
h=\frac{1}{|C|}\sum_{c \in C} W^{(1)}_c
$$

- C: number of context words

Step 2: Multiply by the second weight matrix and do the softmax to get the output prediction
$$
p(y|C)=softmax(W^{(2)T}h)
$$
Note: no hidden activation function!



# Skip-Gram

<img src="3.Word Embeddings and Word2Vec2.png" style="zoom:50%;" />

> "The quick <u>brown fox</u> **jumps** <u>over the</u> lazy dog."

- Take the word 'jump ' as input and try to predict all 4 words
- Like the opposite of CBOW
- Like Bigram neural network which is adding more training samples to it (brown, fox, the)
- The term skip grams -  it's like bigram except we **skip** a few words in between.


$$
h= W^{(1)}_{input} \\
p(y|input)=softmax( W^{(2)T}h)
$$


# Hierarchical Softmax

- Problem: we need to select an output word out of V words
- V can be extremely large
  - pretrained GloVe: V=400,000
  - Pertained word2vec: V=3 million

## Problems

### Large number of output classes

- We know empirically that this negatively affects accuracy
- We have to pick the correct answer out of 3 million possible answers!
  - Consider a softmax picks only 1 class at a time out of 3 million, that's what classification means.
  - 1 is right, 2999999 are wrong
- 99.99997% of the time, any given word is not the target
- We spend the majority of the time pushing the answer away from words than we do trying to get it to the right answer



### Softmax

- Calculating softmax is $O(V)$ (sum)

$$
p(y=j|x)=\frac{\exp{(w_j^Tx)}}{\sum_{k=1}^V\exp{(w_k^Tx)}}
$$



- When you have a large number of output classes it tends not to work as well



## Solution

1. Hierarchical Softmax
2. Negative sampling

**Hierarchical Softmax Idea:**

- Binary tree that splits the vocab
- Leaves are words

<img src="3.Word Embeddings and Word2Vec3.png" style="zoom:50%;" />

## Hierarchical Softmax

1. At every node, we decide whether to go left or right (a binary decision) using sigmoid (this tells us the prob that we should go right or left)
   $$
   p(\text{go right}) = \sigma(v_{node}^Th) \\
   p(\text{go left}) = 1-p(\text{go right})
   $$



2. Output prob is product of every prob on the path to that word
   $$
   p(w|input) =\prod_{node \in \text{ path to w}} \sigma(v_{node}^Th_{input})
   $$

   $$
   \begin{align}
   \text{Ex.} \\
   &p(cat)=0.4 \\
   &p(racoon)=0.6*0.7=0.42 \\
   &p(apple)=0.6*0.3*0.9=0.162 \\
   &Sum=1, \text{just as softmax should}
   \end{align} 
   $$

   

<img src="3.Word Embeddings and Word2Vec4.png" style="zoom:50%;" />

3. Since we have a binary tree, there's no need for a D x V output matrix
   $$
   p(\text{go right}) =\sigma(v_{node}^Th)
   $$
   

- The fact that input weight is V x D is not a problem, since only index it.
- For the output matrix in a naive bigram, we must multiply, because h and $W^{(2)}$ are arbitrary dense matrices.
- Now, each split has a single vector of length D.
- We still have $O(V)$ output weights, but only $O(\log{V})$ operations since this is essentially a binary search



### How do we make this tree?

- We would like 
  - Frequent words to be closer to the TOP
  - Infrequent words to be closer to the bottom
- Huffman coding accomplishes this optimally



# Negative Sampling

With Softmax, most of the time, a word is NOT the target. 

On negative sampling, instead of saying every other word is wrong, we simply **take a sample of these other words and say that some of them are wrong**.



## Multiclass and Binary classification

**Multiclass**: use softmax and multiclass cross-entropy as objective
$$
p(y_{out}|x_{in})=\frac{\exp{(W^{(2)T}_{out}W^{(1)}_{in})}}{\sum_{j=1}^V \exp{(W^{(2)T}_{j}W^{(1)}_{in})}} \\
J=t_{out}\log{p(y_{out}|x_{in})}
$$
**Binary**: use sigmoid and binary cross-entropy as cost 
$$
p(y_{out}=1|x_{in})=\sigma(W^{(2)T}_{out}W^{(1)}_{in}) \\
J=t_{out}\log{p(y_{out}=1|x_{in})}+(1-t_{out})\log{(1-p(y_{out}=1|x_{in}))}
$$


> **Instead of doing multi class cross-entropy, just do binary cross-entropy on the negative samples.**

<img src="3.Word Embeddings and Word2Vec5.png" style="zoom:50%;" />

### Example

> "The quick <u>brown fox</u> **jumps** <u>over the</u> lazy dog."

- Input word: **jumps**

- Target words: brown, fox, over, the

- Negative samples: apple, orange, boat, tokyo

$$
p(brown|jumps)=\sigma(W^{(2)T}_{brown}W^{(1)}_{jumps}) \\
p(fox|jumps)=\sigma(W^{(2)T}_{fox}W^{(1)}_{jumps}) \\
p(over|jumps)=\sigma(W^{(2)T}_{over}W^{(1)}_{jumps}) \\
p(the|jumps)=\sigma(W^{(2)T}_{the}W^{(1)}_{jumps}) \\
p(apple|jumps)=\sigma(W^{(2)T}_{apple}W^{(1)}_{jumps}) \\
p(orange|jumps)=\sigma(W^{(2)T}_{orange}W^{(1)}_{jumps}) \\
p(boat|jumps)=\sigma(W^{(2)T}_{boat}W^{(1)}_{jumps}) \\
p(tokyo|jumps)=\sigma(W^{(2)T}_{tokyo}W^{(1)}_{jumps})
$$

With these 8 posterior probabilities, we could calculate the **binary cross entropy**.
$$
\begin{align}
J&=\log{p(brown|jumps)} +\log{p(fox|jumps)}+ \\
&\log{p(over|jumps)}+\log{p(the|jumps)}+ \\
&\log{[1-p(apple|jumps)]}+\log{[1-p(orange|jumps)]}+ \\
&\log{[1-p(boat|jumps)]}+\log{[1-p(tokyo|jumps)]}
\end{align}
$$

### Objective

$$
J=\sum_{c \in C}\log{\sigma(W^{(2)T}_{c}W^{(1)}_{in})} +\sum_{n \in N}\log{[1-\sigma(W^{(2)T}_{n}W^{(1)}_{in})]}
$$

- C =set of context words
- N=set of ***negative*** samples 

**Another way of writing it**

When it comes to the *sigmoid*, if we negate the **logit** , we get the opposite probability.
$$
p(y=1|x)=\sigma(logit), p(y=0|x)=\sigma(-logit) \\
\text{so,  } \sigma(logit)+\sigma(-logit)=1
$$
 So 
$$
J=\sum_{c \in C}\log{\sigma(W^{(2)T}_{c}W^{(1)}_{in})} +\sum_{n \in N}\log{\sigma(-W^{(2)T}_{n}W^{(1)}_{in})}
$$


## Negative Sampling Gradient

The loss
$$
J=-\sum_{n=1}^N [t_n \log{p_n}+(1-t_n) \log{(1-p_n)}]
$$

- $n$: sample index

- $N$: number of samples in this batch (different from the $N$ above)

- $p_n$: $p(y|x)$, $y$ could refer to any output word

- $t_n$: 1 or 0

  

Gradient wrt $W^{(2)}$ (the same derivative with binary logistic regression)
$$
\frac{\partial J}{\partial W^{(2)}} =H^T(P-T) \\
H=W^{(1)}_{input}
$$

- $P$: a vector containing all the $p_n$  (output probability of the n-th sample).
- $H$: $W^{(1)}_{input}$  - usually it's an $N \times D $ matrix of hidden layer values (for $N$ samples); now it's just **a $D$-length word vector for the single input word**
- $P,T$: vectors of length $N$
- So it doesn't seem like we can multiply these because they are two vectors of different sizes.

### Fixing the problem

#### One sample

- Consider a single sample (N=1) (so $P$ & $T$ are scalars)
- Now this is ok, we can multiple a scalar by a vector
- $W^{(1)}_{input}$ and $W^{(2)}_{output}$ are both vectors of size $D$, the shapes match

<img src="3.Word Embeddings and Word2Vec6.png" style="zoom:50%;" />
$$
\frac{\partial J}{\partial W^{(2)}_{output}} = W^{(1)}_{input}(P-T) \\
$$
We don't want to update the entirety of $W^{(2)}$ because the entirety of $W^{(2)}$ does not influence its output.

In fact only $W^{(2)}_{output}$ affects the current output word whatever it may be.

- Could be a positive sample: t=1
- Or a negative sample: t=0

 

#### Multiple samples

- The slice of $W^{(2)}$ that needs updating is of size <u>D x N</u>
- The outer product of $W^{(1)}_{jumps}$(size $D$) and $P-T$ (size $N$) is <u>D x N</u>
  - N: number of output words, each is represented by a vector of length D
- Given 2 vectors: **a** and **b**, indexed by i and j
  - The outer product **C** = np.outer(**a,b**) is defined as **C**(i,j)=**a**(i)**b**(j)

$$
\frac{\partial J}{\partial W^{(2)}_{brown}} = W^{(1)}_{jumps}(p_{brown}-t_{brown}) \\
\frac{\partial J}{\partial W^{(2)}_{fox}} = W^{(1)}_{jumps}(p_{fox}-t_{fox}) \\
\frac{\partial J}{\partial W^{(2)}_{over}} = W^{(1)}_{jumps}(p_{over}-t_{over}) \\
\frac{\partial J}{\partial W^{(2)}_{C}} = np.outer(W^{(1)}_{jumps},(p_{C}-t_{C})), C= {brown,fox,over} \\
$$

### Updating the input weight

 Derivative up to logit/ final activation
$$
\begin{align}
J&=-\sum_{n=1}^N [t_n \log{\sigma(a_n)}+(1-t_n) \log{(1-\sigma(a_n))}] \\
\frac{\partial J}{\partial a_n} &= -\frac{\partial }{\partial a_n} [t_n \log{\sigma(a_n)}+(1-t_n) \log{(1-\sigma(a_n))}] \\
&=-\frac{t_n}{\sigma(a_n)}\frac{\partial \sigma(a_n)}{\partial a_n}+\frac{1-t_n}{1-\sigma(a_n)}\frac{\partial \sigma(a_n)}{\partial a_n}\\

&=[-\frac{t_n}{\sigma(a_n)}+\frac{1-t_n}{1-\sigma(a_n)}]\sigma(a_n)[1-\sigma(a_n)] \\
&=-t_n[1-\sigma(a_n)]+(1-t_n)\sigma(a_n) \\
&=\sigma(a_n)-t_n \\
&=p_n-t_n \\
a_n &=W^{(2)T}_{out(n)}W^{(1)}_{in}
\end{align} \\
$$
<img src="3.Word Embeddings and Word2Vec7.png" style="zoom:50%;" />
$$
\begin{align} \\
\frac{\partial J}{\partial W^{(1)}_{in}} &=\sum_{i=1}^N \frac{\partial J}{\partial a_n} \frac{\partial a_n}{\partial W^{(1)}_{in}} \\
&=\sum_{i=1}^N (p_n-t_n) \frac{\partial W^{(2)T}_{out(n)}W^{(1)}_{in}}{\partial W^{(1)}_{in}} \\
&=\sum_{i=1}^N (p_n-t_n) W^{(2)}_{out(n)}
\end{align} \\
$$

# Summary

- Word2vec is just some modifications on the bigram neural network

| Modeling | Approximating Softmax |
| -------- | --------------------- |
| CBOW     | Hierarchical Softmax  |
| Skipgram | Negative Sampling     |







# Negative Sampling - Details

1. How many negative samples should you choose?

- 5 or 10

2. How do you actually choose the negative samples ?

- Did you assume a uniform distribution? Wrong!



**Modified unigram distribution**

- **Unigram**: just the probability of a single word occurring - $p(w)$

$$
p(w)=\frac{count(w)}{\sum_{w^{'}}count(w^{'})}
$$

- Infrequent words are too infrequent and so they are very unlikely to ever be sampled.
  $$
  \tilde{p}(w)=\frac{count(w)^{0.75}}{\sum_{w^{'}}count(w^{'})^{0.75}}
  $$
  

3. What if during my negative sampling procedure I accidentally sample one of the context words.
   - It's ok, treat it like a negative sample too
   - Too much work to ensure we don't accidentally sample context



4. Usually when people talk about negative sampling they talk about it in terms of sampling **negative context words** with **a fixed middle word**. During implementation we're going to **fix the context words** and **insert an incorrect middle word**.



# Why do I have 2 word embedding matrices ?

## 2 possible word embeddings

- All we want to do is find a $V \times D$ matrix that has good word vectors.

- Look at the architecture of a bigram neural network, it has 2 word embeddings.

<img src="3.Word Embeddings and Word2Vec8.png" style="zoom:50%;" />

## Which one should you choose?

**Option 1**: just make the word embedding equal to the input hidden weight

$ W_e=W^{(1)}$

**Option 2**: concatenate the two weights together.

$W_e=concat([W^{(1)},W^{(2)}]) \rightarrow \text{shape is } V \times 2D$

**Option 3**: take the average of the two weights

$W_e=(W^{(1)}+W^{(2)T})/2$



### Normalization

- Sometimes people normalize all the vectors so that they have unit length.

$$
\hat{v}=\frac{v}{|v|}
$$





# Implementation Tricks

## How to choose words to train on?

- Word frequencies follow a fat-tailed distribution
  - common words are very common and uncommon words are very uncommon.
- Solution? Drop a bunch of words randomly

$$
p_{drop}(w)=1-\sqrt{\frac{threshold}{\tilde{p}(w)}} \tag{prob of dropping a word}
$$

- Typical threshold = $10^{-5}$
  - Ex. if $p(w)=10^{-5}$, then $p_{drop}(w)=1-1=0$
  - Ex. if $p(w)=0.1$, then $p_{drop}(w)=1-10^{-2}=0.99$



After we drop the words from the sentence we treat the sentence with dropped words as the final sentence that we're going to train on.

- Ex. if we drop "over" and "the" so that:

  "The quick <u>brown fox</u> **jumps** <u>over the</u> lazy dog." becomes

  "Quick brown fox **jumps** over  lazy dog."

 ***This has the effect of widening the context window***





## Learning rate scheduling

In words2vec, we normally pick a **maximum learning rate** and a **minimum learning rate** and then we reduce the learning rate linearly from the maximum to the minimum.



## Interesting

- **Parallel processing**: greatly speed up training by working on multiple threads or multiple cores (each responsible for a different subset of sentences).
- Using C language





