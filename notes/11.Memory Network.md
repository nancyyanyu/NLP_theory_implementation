<img src="11.Memory Network1.png" style="zoom:50%;" />

<img src="11.Memory Network2.png" style="zoom:50%;" />

<img src="11.Memory Network3.png" style="zoom:50%;" />





# Theory

- Recall: a story is a list of sequences
- Simplest thing: sum up all word vectors to get a setence vector ("bag of words")

<img src="11.Memory Network4.png" style="zoom:20%;" />

- I have a vector for each sentence. Now I want to figure out which sentence should I pay **attention** to to determine the answer to the question.

**How do we find the weights for each sentence?**

1. Convert the *question* which is also a sequence of words into **a sequence of vectors**
2. Sum those vectors into one factor.
   - Same process with what we did for the story but importantly the *question* is going to have its own **embedding**
3. **Dot** the *question* vector with the *sentence* vector and then take the `softmax` of all those outputs.
   - $w_i=softmax(q^Ts_i)$

<img src="11.Memory Network5.png" style="zoom:30%;" />



## Sentence weights

> Why does "dot" make sense?

- It's a "distance finder"
- Embeddings in memory networks are **initialized randomly** and separately learned for both stories and questions from scratch, since they need to have this special relationship to one another.
- Questions embeddings will learn to be "close" to relevant story sentences
- Weights depend on the *question*, and tell us how **relevant** each sentence is for answering the *question*.



## After getting the weights

Now that we have these weights we can apply them to the *sentence vectors* and get out a **new vector** which you can more or less think of as ***a vector representing the relevant sentence***.

Next,

1. Pass this vector through a logistic regression (a single dense layer)
2. Apply a final `softmax`
   -  output size =  vocab size
   - The *answer* is always a single word.

<img src="11.Memory Network6.png" style="zoom:50%;" />

**Disadvantage**: can't respond in sentences.



# 2 Supporting Fact

> Why doesn't our existing model work?

- Softmax can only pick 1 thing!
- In addition, order matters:
  - "John goes to the kitchen."
  - "John goes to the hallway."
  - "Where is John" --- Depends on which order the story happened!

## Strategy

- Simply make 2 of the same block!
  - 1 designed to find the first fact
  - 1 desdigned to fine the other fact

> How do we prevent both of these blocks from simply returning the same answer in both cases?

- "Sort of" recurrent
- Solution: Pass the output of first block to the second!

<img src="11.Memory Network7.png" style="zoom:50%;" />



## Small details

- Where do we need story embeddings?
  1. When we create the weights for each story line (to be dotted with question embedding)
  2. When having vectors to actually represent each storyline. What we multiply the weights by later
- Tip: use 2 separate embeddings
  - Pass 2nd embedding from 1st hop to be used for createing weights in 2nd hop
  - Not needed to do well on single supporting fact
- Tip: add dense layer as part of hop, after getting weighted sum of sentence vectors($s_{relevant}$)
  - Square matrix so vectors remain the same size



## Model interpretation

- Similar to attention, you can save the weights of each storyline
- Will tell us which line the model found most important 
- Fast training on CPU



# Summary

-  Before memory networks
  - No memory - just read the input sequence and produce an answer
- With memory networks
  - Holds a memory of the story it read

## Memory Networks Section

- Not just reading a story, but also figuring out which part is relevant to the current question
- Multiple questions for same story would have different answers
- Is necessarily contrived, since we must go from sinple to complex

<img src="11.Memory Network8.png" style="zoom:50%;" />

- Is bag-of-words limiting? Can we use RNNs? Sure!
- Each sentence in story -> RNN -> Single vector
- Question -> RNN -> Single vector
- Attention
- Usee RNN for hops
- "Dynamic" memory networks

