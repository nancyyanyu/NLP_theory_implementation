{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T15:48:15.279363Z",
     "start_time": "2020-09-11T15:48:14.505671Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_wikipedia_data\n",
    "from nlp_class2.util import find_analogies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and Assess Word Vectors using TF-IDF and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T21:24:37.735638Z",
     "start_time": "2020-09-10T21:24:37.732731Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_analogies_TFIDF_tSNE():\n",
    "    analogies_to_try = (\n",
    "        ('king', 'man', 'woman'),\n",
    "        ('france', 'paris', 'london'),\n",
    "        ('france', 'paris', 'rome'),\n",
    "        ('paris', 'france', 'italy'),\n",
    "    )\n",
    "\n",
    "    sentences, word2idx = get_wikipedia_data(\n",
    "        n_files=3, n_vocab=2000, by_paragraph=True)\n",
    "\n",
    "    notfound = False\n",
    "    for word_list in analogies_to_try:\n",
    "        for word in word_list:\n",
    "            if word not in word2idx:\n",
    "                notfound = True\n",
    "    if notfound:\n",
    "        exit()\n",
    "\n",
    "    # build term document matrix\n",
    "    V = len(word2idx)\n",
    "    N = len(sentences)\n",
    "\n",
    "    # create raw counts first\n",
    "    A = np.zeros((V, N))\n",
    "    print(\"V:\", V, \"N:\", N)\n",
    "\n",
    "    j = 0\n",
    "    for sentence in sentences:\n",
    "        for i in sentence:\n",
    "            A[i, j] += 1\n",
    "        j += 1\n",
    "    print(\"finished getting raw counts\")\n",
    "\n",
    "    transformer = TfidfTransformer()\n",
    "    A = transformer.fit_transform(A.T).T\n",
    "    A = A.toarray()\n",
    "\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    tsne = TSNE()\n",
    "    Z = tsne.fit_transform(A)\n",
    "\n",
    "    tsne2 = TSNE(n_components=3)\n",
    "    We = tsne.fit_transform(A)\n",
    "\n",
    "    plt.scatter(Z[:, 0], Z[:, 1])\n",
    "    for i in range(V):\n",
    "        try:\n",
    "            plt.annotate(s=idx2word[i].encode(\n",
    "                'utf-8').decode('utf-8'), xy=(Z[i, 0], Z[i, 1]))\n",
    "        except:\n",
    "            pass\n",
    "    plt.draw()\n",
    "\n",
    "\n",
    "    for word_list in analogies_to_try:\n",
    "        w1, w2, w3 = word_list\n",
    "        find_analogies(w1, w2, w3, We, word2idx, idx2word)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Trained Word Vectors from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T15:48:18.423007Z",
     "start_time": "2020-09-11T15:48:18.420529Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T15:48:23.971107Z",
     "start_time": "2020-09-11T15:48:18.736607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "embedding = []\n",
    "idx2word = []\n",
    "with open('./large_files/glove.6B/glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.array(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "        embedding.append(vec)\n",
    "        idx2word.append(word)\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "embedding = np.array(embedding)\n",
    "V, D = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T15:54:09.055686Z",
     "start_time": "2020-09-11T15:48:27.903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50, 400000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, D, len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-11T15:46:59.173Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_analogies_GloVe(w1,w2,w3):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word2vec:\n",
    "            print(\"{} not in word2vec\".format(w))\n",
    "\n",
    "    king = word2vec[w1]\n",
    "    man = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    v0 = king-man+woman\n",
    "    distances = pairwise_distances(v0.reshape(1,D), embedding, metric='cosine')\n",
    "    distances = distances.reshape(V)\n",
    "    idxs = distances.argsort()[:4]\n",
    "    for idx in idxs:\n",
    "        word = idx2word[idx]\n",
    "        if  word not in (w1,w2,w3):\n",
    "            best_word = word\n",
    "            break\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = britain - london\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n",
      "france - french = england - english\n",
      "japan - japanese = china - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = australia - australian\n",
      "december - november = july - june\n",
      "miami - florida = houston - texas\n",
      "einstein - scientist = matisse - painter\n",
      "china - rice = chinese - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = friend - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = queen - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = china - beijing\n",
      "february - january = october - november\n",
      "france - paris = italy - rome\n",
      "paris - france = rome - italy\n"
     ]
    }
   ],
   "source": [
    "find_analogies_GloVe('king', 'man', 'woman')\n",
    "find_analogies_GloVe('france', 'paris', 'london')\n",
    "find_analogies_GloVe('france', 'paris', 'rome')\n",
    "find_analogies_GloVe('paris', 'france', 'italy')\n",
    "find_analogies_GloVe('france', 'french', 'english')\n",
    "find_analogies_GloVe('japan', 'japanese', 'chinese')\n",
    "find_analogies_GloVe('japan', 'japanese', 'italian')\n",
    "find_analogies_GloVe('japan', 'japanese', 'australian')\n",
    "find_analogies_GloVe('december', 'november', 'june')\n",
    "find_analogies_GloVe('miami', 'florida', 'texas')\n",
    "find_analogies_GloVe('einstein', 'scientist', 'painter')\n",
    "find_analogies_GloVe('china', 'rice', 'bread')\n",
    "find_analogies_GloVe('man', 'woman', 'she')\n",
    "find_analogies_GloVe('man', 'woman', 'aunt')\n",
    "find_analogies_GloVe('man', 'woman', 'sister')\n",
    "find_analogies_GloVe('man', 'woman', 'wife')\n",
    "find_analogies_GloVe('man', 'woman', 'actress')\n",
    "find_analogies_GloVe('man', 'woman', 'mother')\n",
    "find_analogies_GloVe('heir', 'heiress', 'princess')\n",
    "find_analogies_GloVe('nephew', 'niece', 'aunt')\n",
    "find_analogies_GloVe('france', 'paris', 'tokyo')\n",
    "find_analogies_GloVe('france', 'paris', 'beijing')\n",
    "find_analogies_GloVe('february', 'january', 'november')\n",
    "find_analogies_GloVe('france', 'paris', 'rome')\n",
    "find_analogies_GloVe('paris', 'france', 'italy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nearest_neighbors_GloVe(w, n=5):\n",
    "    print(w)\n",
    "    if w not in word2vec:\n",
    "        print(\"{} not in word2vec\".format(w))\n",
    "    v=word2vec[w]\n",
    "    distance = pairwise_distances(v.reshape(1,D),embedding,metric='cosine').reshape(V)\n",
    "    idxs = distance.argsort()[1:n+1]\n",
    "    for idx in idxs:\n",
    "         print(\"\\t%s\" % idx2word[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "\tprince\n",
      "\tqueen\n",
      "\tii\n",
      "\temperor\n",
      "\tson\n",
      "\n",
      "france\n",
      "\tfrench\n",
      "\tbelgium\n",
      "\tparis\n",
      "\tspain\n",
      "\tnetherlands\n",
      "\n",
      "japan\n",
      "\tjapanese\n",
      "\tchina\n",
      "\tkorea\n",
      "\ttokyo\n",
      "\ttaiwan\n",
      "\n",
      "einstein\n",
      "\trelativity\n",
      "\tbohr\n",
      "\tphysics\n",
      "\theisenberg\n",
      "\tfreud\n",
      "\n",
      "woman\n",
      "\tgirl\n",
      "\tman\n",
      "\tmother\n",
      "\ther\n",
      "\tboy\n",
      "\n",
      "nephew\n",
      "\tcousin\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tson\n",
      "\tuncle\n",
      "\n",
      "february\n",
      "\toctober\n",
      "\tdecember\n",
      "\tjanuary\n",
      "\taugust\n",
      "\tseptember\n",
      "\n",
      "rome\n",
      "\tnaples\n",
      "\tvenice\n",
      "\titaly\n",
      "\tturin\n",
      "\tpope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors_GloVe('king')\n",
    "nearest_neighbors_GloVe('france')\n",
    "nearest_neighbors_GloVe('japan')\n",
    "nearest_neighbors_GloVe('einstein')\n",
    "nearest_neighbors_GloVe('woman')\n",
    "nearest_neighbors_GloVe('nephew')\n",
    "nearest_neighbors_GloVe('february')\n",
    "nearest_neighbors_GloVe('rome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Trained Word Vectors from Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors=KeyedVectors.load_word2vec_format('./large_files/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies_word2vec(w1,w2,w3):\n",
    "    r = word_vectors.most_similar(positive=[w1,w3],negative=[w2])\n",
    "    print(\"%s - %s = %s - %s\" % (w1, w2, r[0][0], w3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man = queen - woman\n",
      "france - paris = england - london\n",
      "france - paris = italy - rome\n",
      "paris - france = lohan - italy\n",
      "france - french = england - english\n",
      "japan - japanese = tibet - chinese\n",
      "japan - japanese = italy - italian\n",
      "japan - japanese = queensland - australian\n",
      "december - november = september - june\n",
      "miami - florida = dallas - texas\n",
      "einstein - scientist = jude - painter\n",
      "china - rice = dinnerware - bread\n",
      "man - woman = he - she\n",
      "man - woman = uncle - aunt\n",
      "man - woman = brother - sister\n",
      "man - woman = son - wife\n",
      "man - woman = actor - actress\n",
      "man - woman = father - mother\n",
      "heir - heiress = prince - princess\n",
      "nephew - niece = uncle - aunt\n",
      "france - paris = japan - tokyo\n",
      "france - paris = chinese - beijing\n",
      "february - january = april - november\n",
      "france - paris = italy - rome\n",
      "paris - france = lohan - italy\n"
     ]
    }
   ],
   "source": [
    "find_analogies_word2vec('king', 'man', 'woman')\n",
    "find_analogies_word2vec('france', 'paris', 'london')\n",
    "find_analogies_word2vec('france', 'paris', 'rome')\n",
    "find_analogies_word2vec('paris', 'france', 'italy')\n",
    "find_analogies_word2vec('france', 'french', 'english')\n",
    "find_analogies_word2vec('japan', 'japanese', 'chinese')\n",
    "find_analogies_word2vec('japan', 'japanese', 'italian')\n",
    "find_analogies_word2vec('japan', 'japanese', 'australian')\n",
    "find_analogies_word2vec('december', 'november', 'june')\n",
    "find_analogies_word2vec('miami', 'florida', 'texas')\n",
    "find_analogies_word2vec('einstein', 'scientist', 'painter')\n",
    "find_analogies_word2vec('china', 'rice', 'bread')\n",
    "find_analogies_word2vec('man', 'woman', 'she')\n",
    "find_analogies_word2vec('man', 'woman', 'aunt')\n",
    "find_analogies_word2vec('man', 'woman', 'sister')\n",
    "find_analogies_word2vec('man', 'woman', 'wife')\n",
    "find_analogies_word2vec('man', 'woman', 'actress')\n",
    "find_analogies_word2vec('man', 'woman', 'mother')\n",
    "find_analogies_word2vec('heir', 'heiress', 'princess')\n",
    "find_analogies_word2vec('nephew', 'niece', 'aunt')\n",
    "find_analogies_word2vec('france', 'paris', 'tokyo')\n",
    "find_analogies_word2vec('france', 'paris', 'beijing')\n",
    "find_analogies_word2vec('february', 'january', 'november')\n",
    "find_analogies_word2vec('france', 'paris', 'rome')\n",
    "find_analogies_word2vec('paris', 'france', 'italy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors_word2vec(w,n=5):\n",
    "    print(w)\n",
    "    r = word_vectors.most_similar(positive=[w])\n",
    "    count=0\n",
    "    for word,score in r:\n",
    "        if count<=5:\n",
    "            print(\"\\t%s\" % word)\n",
    "            count+=1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "\tkings\n",
      "\tqueen\n",
      "\tmonarch\n",
      "\tcrown_prince\n",
      "\tprince\n",
      "\tsultan\n",
      "france\n",
      "\tspain\n",
      "\tfrench\n",
      "\tgermany\n",
      "\teurope\n",
      "\titaly\n",
      "\tengland\n",
      "japan\n",
      "\tjapanese\n",
      "\ttokyo\n",
      "\tamerica\n",
      "\teurope\n",
      "\tgermany\n",
      "\tchinese\n",
      "einstein\n",
      "\tnikki\n",
      "\tlmfao\n",
      "\talbert\n",
      "\tarmstrong\n",
      "\tjoan\n",
      "\tbecky\n",
      "woman\n",
      "\tman\n",
      "\tgirl\n",
      "\tteenage_girl\n",
      "\tteenager\n",
      "\tlady\n",
      "\tteenaged_girl\n",
      "nephew\n",
      "\tson\n",
      "\tuncle\n",
      "\tbrother\n",
      "\tgrandson\n",
      "\tcousin\n",
      "\tfather\n",
      "february\n",
      "\tjanuary\n",
      "\tapril\n",
      "\tseptember\n",
      "\tdecember\n",
      "\tjuly\n",
      "\toctober\n",
      "rome\n",
      "\tathens\n",
      "\talbert\n",
      "\tholmes\n",
      "\tdonnie\n",
      "\titaly\n",
      "\ttoni\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbors_word2vec('king')\n",
    "nearest_neighbors_word2vec('france')\n",
    "nearest_neighbors_word2vec('japan')\n",
    "nearest_neighbors_word2vec('einstein')\n",
    "nearest_neighbors_word2vec('woman')\n",
    "nearest_neighbors_word2vec('nephew')\n",
    "nearest_neighbors_word2vec('february')\n",
    "nearest_neighbors_word2vec('rome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('./large_files/r8-train-all-terms.txt',sep='\\t',header=None)\n",
    "test=pd.read_csv('./large_files/r8-test-all-terms.txt',sep='\\t',header=None)\n",
    "train.columns=['label','content']\n",
    "test.columns=['label','content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earn</td>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earn</td>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earn</td>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earn</td>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0  earn  champion products ch approves stock split cham...\n",
       "1   acq  computer terminal systems cpml completes sale ...\n",
       "2  earn  cobanco inc cbco year net shr cts vs dlrs net ...\n",
       "3  earn  am international inc am nd qtr jan oper shr lo...\n",
       "4  earn  brown forman inc bfd th qtr net shr one dlr vs..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(object):\n",
    "    def __init__(self):\n",
    "        print('Loading word vectors...')\n",
    "        self.word2vec = {}\n",
    "        self.embedding = []\n",
    "        self.idx2word = []\n",
    "        with open('./large_files/glove.6B/glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.array(values[1:], dtype='float32')\n",
    "                self.word2vec[word] = vec\n",
    "                self.embedding.append(vec)\n",
    "                self.idx2word.append(word)\n",
    "        print('Found %s word vectors.' % len(self.word2vec))\n",
    "        self.embedding = np.array(self.embedding)\n",
    "        self.V, self.D = self.embedding.shape\n",
    "        self.word2idx={v:k for k,v in enumerate(idx2word)}\n",
    "        \n",
    "    def fit(self,data):\n",
    "        pass\n",
    "    \n",
    "    def transform(self,data):\n",
    "        X=np.zeros((len(data),self.D))\n",
    "        n=0\n",
    "        emptycount=0\n",
    "        for sentence in data:\n",
    "            tokens=sentence.lower().split()\n",
    "            vecs=[]\n",
    "            for word in tokens:\n",
    "                if word in self.word2vec:\n",
    "                    vec=self.word2vec[word]\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs)>0:\n",
    "                vecs=np.array(vecs)\n",
    "                X[n]=vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount+=1\n",
    "            n+=1\n",
    "        print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "        \n",
    "    def fit_transform(self,data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vecVectorizer(object):\n",
    "    def __init__(object):\n",
    "        print(\"Loading in word vectors...\")\n",
    "        self.word_vectors=KeyedVectors.load_word2vec_format('./large_files/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "        print(\"Finished loading in word vectors\")\n",
    "        \n",
    "    def fit(self,data):\n",
    "        pass\n",
    "    \n",
    "    def transform(self,data):\n",
    "        v=self.word_vectors.get_vector('king')\n",
    "        self.D=v.shape[0]\n",
    "        X=np.zeros((len(data),self.D))\n",
    "        n=0\n",
    "        emptycount=0\n",
    "        for sentence in data:\n",
    "            tokens=sentence.lowe().split()\n",
    "            vecs=[]\n",
    "            for word in tokens:\n",
    "                if word in self.word_vectors:\n",
    "                    vec = self.word_vectors.get_vector(word)\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs)>0:\n",
    "                X[n]=np.array(vecs).mean(axis=0)\n",
    "            else:\n",
    "                emptycount+=1\n",
    "            n+=1\n",
    "            print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "        \n",
    "    def fit_transform(self,data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n",
      "Numer of samples with no words found: 0 / 5485\n",
      "Numer of samples with no words found: 0 / 2189\n"
     ]
    }
   ],
   "source": [
    "vectorizer_glove=GloveVectorizer()\n",
    "X_train = vectorizer_glove.fit_transform(train.content)\n",
    "y_train = train.label\n",
    "X_test = vectorizer_glove.fit_transform(test.content)\n",
    "y_test = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n",
      "Numer of samples with no words found: 0 / 5485\n",
      "Numer of samples with no words found: 0 / 2189\n"
     ]
    }
   ],
   "source": [
    "vectorizer_w2v=GloveVectorizer()\n",
    "X_train2 = vectorizer_w2v.fit_transform(train.content)\n",
    "y_train2 = train.label\n",
    "X_test2 = vectorizer_w2v.fit_transform(test.content)\n",
    "y_test2 = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9992707383773929\n",
      "test score:  0.9328460484239379\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf.fit(X_train,y_train)\n",
    "print(\"train score: \",rf.score(X_train,y_train))\n",
    "print(\"test score: \",rf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9992707383773929\n",
      "test score:  0.9328460484239379\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf.fit(X_train2,y_train2)\n",
    "print(\"train score: \",rf.score(X_train2,y_train2))\n",
    "print(\"test score: \",rf.score(X_test2,y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9992707383773929\n",
      "test score:  0.9378711740520785\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=200)\n",
    "et.fit(X_train,y_train)\n",
    "print(\"train score: \",et.score(X_train,y_train))\n",
    "print(\"test score: \",et.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9992707383773929\n",
      "test score:  0.9337597076290544\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=200)\n",
    "et.fit(X_train2,y_train2)\n",
    "print(\"train score: \",et.score(X_train2,y_train2))\n",
    "print(\"test score: \",et.score(X_test2,y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
