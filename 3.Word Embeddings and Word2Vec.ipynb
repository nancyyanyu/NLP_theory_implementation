{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wiki(V=20000,n=None):\n",
    "    all_words_count={}\n",
    "    f ='./large_files/enwiki-preprocessed/'\n",
    "    files = os.listdir(f)[:n]\n",
    "    for file in files:\n",
    "        for line in open(f+file):\n",
    "            if line and line[0] not in '[*-|=\\{\\}':\n",
    "                s = line.translate(str.maketrans('','',string.punctuation)).lower().split()\n",
    "                if len(s)>1:\n",
    "                    for word in s:\n",
    "                        if word not in all_words_count:\n",
    "                            all_words_count[word]=0\n",
    "                        all_words_count[word]+=1\n",
    "\n",
    "\n",
    "    V=min(V, len(all_words_count))\n",
    "\n",
    "    all_words_count = sorted(all_words_count.items(),key=lambda x: x[1],reverse=True)\n",
    "    top_words = [w for w,c in all_words_count[:V-1]]+['<UNK>']\n",
    "    word2idx={w:i for i,w in enumerate(top_words)}\n",
    "    unk=word2idx['<UNK>']\n",
    "\n",
    "    sents=[]\n",
    "    for file in files:\n",
    "        for line in open(f+file):\n",
    "            if line and line[0] not in '[*-|=\\{\\}':\n",
    "                s = line.translate(str.maketrans('','',string.punctuation)).lower().split()\n",
    "                if len(s)>1:\n",
    "                    sent = [word2idx[word] if word in word2idx else unk for word in s]\n",
    "                    sents.append(sent)\n",
    "    return sents,word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sgd(word,targets,label,lr,W1,W2):\n",
    "    # W1[input_] shape: D\n",
    "    # W2[:,targets] shape: D x Number of context words\n",
    "    # activation shape: N\n",
    "    # print(\"input_:\", input_, \"targets:\", targets)\n",
    "    a_n = W2[:,targets].T.dot(W1[word]) # N,\n",
    "    p_n= sigmoid(a_n) # N,\n",
    "    gW2 = np.outer(W1[word],p_n-label) # D x N\n",
    "    gW1 = ((p_n-label) *(W2[:,targets])).sum(axis=1) # D \n",
    "\n",
    "    W2[:,targets]-=lr*gW2 # D x N\n",
    "    W1[word]-=lr*gW1 # D \n",
    "\n",
    "    # return cost (binary cross entropy)\n",
    "    cost = label*np.log(p_n+1e-10)+(1-label)*np.log(1-p_n+1e-10)\n",
    "    return cost.sum()\n",
    "\n",
    "def get_context(pos,sentence,window_size):\n",
    "    # input:\n",
    "    # a sentence of the form: x x x x c c c pos c c c x x x x\n",
    "    # output:\n",
    "    # the context word indices: c c c c c c\n",
    "    start=max(0,pos-window_size)\n",
    "    end = min(len(sentence),pos+window_size)\n",
    "    context=[]\n",
    "\n",
    "    for i ,w in enumerate(sentence[start:end],start=start):\n",
    "        if i!=pos:\n",
    "            context.append(w)\n",
    "    return context\n",
    "\n",
    "def neg_sampling_dstn(sentences,vocab_size):\n",
    "    # Pn(w) = prob of word occuring\n",
    "    # we would like to sample the negative samples\n",
    "    # such that words that occur more often\n",
    "    # should be sampled more often\n",
    "\n",
    "    word_freq=np.zeros(vocab_size)\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            word_freq[word]+=1\n",
    "    p_neg = word_freq**0.75/ (word_freq**0.75).sum()\n",
    "    return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n):\n",
    "    sentences,word2idx = read_wiki(20000,n)\n",
    "\n",
    "    vocab_size=len(word2idx)\n",
    "\n",
    "    window_size=4\n",
    "    lr=0.025\n",
    "    final_lr=0.0001\n",
    "    num_neg=5 # number of negative samples to draw per input word\n",
    "    epoches=20\n",
    "    D=50  # word embedding size\n",
    "\n",
    "    # learning rate decay\n",
    "    lr_delta=(lr-final_lr)/epoches\n",
    "\n",
    "    W1=np.random.randn(vocab_size,D) # input-to-hidden\n",
    "    W2=np.random.randn(D,vocab_size) # hidden-to-output\n",
    "\n",
    "    # distribution for drawing negative samples\n",
    "    p_neg = neg_sampling_dstn(sentences,vocab_size)\n",
    "\n",
    "    costs = []\n",
    "    total_words = sum(len(sent) for sent in sentences)\n",
    "    print(\"Total words: \",total_words)\n",
    "\n",
    "    threshold=1e-5\n",
    "    p_drop=1-np.sqrt(threshold/p_neg)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        random.shuffle(sentences)\n",
    "        cost=0\n",
    "        counter=0\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            sentence = [w for w in sentence if np.random.random()<1-p_drop[w]]\n",
    "\n",
    "            if len(sentence)<2: continue\n",
    "                \n",
    "            # randomly order words so we don't always see samples in the same order\n",
    "            randomly_ordered_sentence = np.random.choice(len(sentence),size = len(sentence),replace=False)\n",
    "\n",
    "            for pos in randomly_ordered_sentence:\n",
    "                # the middle word\n",
    "                word = sentence[pos]\n",
    "                # get the positive context words/negative samples\n",
    "                context_words = get_context(pos,sentence,window_size)\n",
    "                neg_word = np.random.choice(vocab_size,p=p_neg)\n",
    "                targets=np.array(context_words)\n",
    "\n",
    "                # do one iteration of stochastic gradient descent\n",
    "                c=sgd(word,targets,1,lr,W1,W2)\n",
    "                cost+=c\n",
    "                c=sgd(neg_word,targets,0,lr,W1,W2)\n",
    "                cost+=c\n",
    "            counter+=1\n",
    "            if counter % 2000 == 0:\n",
    "                print(\"  processed %s / %s\\r\" % (counter, len(sentences)))\n",
    "        print(\"epoch complete:\", epoch, \"cost:\", cost)\n",
    "\n",
    "\n",
    "        # save the cost\n",
    "        costs.append(cost)\n",
    "\n",
    "        # update the learning rate\n",
    "        lr -= lr_delta\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "\n",
    "    if not os.path.isdir('./model'):\n",
    "        os.mkdir('./model')\n",
    "\n",
    "    with open('./model/word2vec.json','w') as f:\n",
    "        json.dump(word2idx,f)\n",
    "\n",
    "    np.savez('./model/weights.npz',W1,W2)\n",
    "    print(\"saved model\")\n",
    "    return word2idx, W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  86478677\n",
      "  processed 2000 / 1271558\n",
      "  processed 4000 / 1271558\n",
      "  processed 6000 / 1271558\n",
      "  processed 8000 / 1271558\n",
      "  processed 10000 / 1271558\n",
      "  processed 12000 / 1271558\n",
      "  processed 14000 / 1271558\n",
      "  processed 16000 / 1271558\n",
      "  processed 18000 / 1271558\n",
      "  processed 20000 / 1271558\n",
      "  processed 22000 / 1271558\n",
      "  processed 24000 / 1271558\n",
      "  processed 26000 / 1271558\n",
      "  processed 28000 / 1271558\n",
      "  processed 30000 / 1271558\n",
      "  processed 32000 / 1271558\n",
      "  processed 34000 / 1271558\n",
      "  processed 36000 / 1271558\n",
      "  processed 38000 / 1271558\n",
      "  processed 40000 / 1271558\n",
      "  processed 42000 / 1271558\n",
      "  processed 44000 / 1271558\n",
      "  processed 46000 / 1271558\n",
      "  processed 48000 / 1271558\n",
      "  processed 50000 / 1271558\n",
      "  processed 52000 / 1271558\n",
      "  processed 54000 / 1271558\n",
      "  processed 56000 / 1271558\n",
      "  processed 58000 / 1271558\n",
      "  processed 60000 / 1271558\n",
      "  processed 62000 / 1271558\n",
      "  processed 64000 / 1271558\n",
      "  processed 66000 / 1271558\n",
      "  processed 68000 / 1271558\n",
      "  processed 70000 / 1271558\n",
      "  processed 72000 / 1271558\n",
      "  processed 74000 / 1271558\n",
      "  processed 76000 / 1271558\n",
      "  processed 78000 / 1271558\n",
      "  processed 80000 / 1271558\n",
      "  processed 82000 / 1271558\n",
      "  processed 84000 / 1271558\n",
      "  processed 86000 / 1271558\n",
      "  processed 88000 / 1271558\n",
      "  processed 90000 / 1271558\n",
      "  processed 92000 / 1271558\n",
      "  processed 94000 / 1271558\n",
      "  processed 96000 / 1271558\n",
      "  processed 98000 / 1271558\n",
      "  processed 100000 / 1271558\n",
      "  processed 102000 / 1271558\n",
      "  processed 104000 / 1271558\n",
      "  processed 106000 / 1271558\n",
      "  processed 108000 / 1271558\n",
      "  processed 110000 / 1271558\n",
      "  processed 112000 / 1271558\n",
      "  processed 114000 / 1271558\n",
      "  processed 116000 / 1271558\n",
      "  processed 118000 / 1271558\n",
      "  processed 120000 / 1271558\n",
      "  processed 122000 / 1271558\n",
      "  processed 124000 / 1271558\n",
      "  processed 126000 / 1271558\n",
      "  processed 128000 / 1271558\n",
      "  processed 130000 / 1271558\n",
      "  processed 132000 / 1271558\n",
      "  processed 134000 / 1271558\n",
      "  processed 136000 / 1271558\n",
      "  processed 138000 / 1271558\n",
      "  processed 140000 / 1271558\n",
      "  processed 142000 / 1271558\n",
      "  processed 144000 / 1271558\n",
      "  processed 146000 / 1271558\n",
      "  processed 148000 / 1271558\n",
      "  processed 150000 / 1271558\n",
      "  processed 152000 / 1271558\n",
      "  processed 154000 / 1271558\n",
      "  processed 156000 / 1271558\n",
      "  processed 158000 / 1271558\n",
      "  processed 160000 / 1271558\n",
      "  processed 162000 / 1271558\n",
      "  processed 164000 / 1271558\n",
      "  processed 166000 / 1271558\n",
      "  processed 168000 / 1271558\n",
      "  processed 170000 / 1271558\n",
      "  processed 172000 / 1271558\n",
      "  processed 174000 / 1271558\n",
      "  processed 176000 / 1271558\n",
      "  processed 178000 / 1271558\n",
      "  processed 180000 / 1271558\n",
      "  processed 182000 / 1271558\n",
      "  processed 184000 / 1271558\n",
      "  processed 186000 / 1271558\n",
      "  processed 188000 / 1271558\n",
      "  processed 190000 / 1271558\n",
      "  processed 192000 / 1271558\n",
      "  processed 194000 / 1271558\n",
      "  processed 196000 / 1271558\n",
      "  processed 198000 / 1271558\n",
      "  processed 200000 / 1271558\n",
      "  processed 202000 / 1271558\n",
      "  processed 204000 / 1271558\n",
      "  processed 206000 / 1271558\n",
      "  processed 208000 / 1271558\n",
      "  processed 210000 / 1271558\n",
      "  processed 212000 / 1271558\n",
      "  processed 214000 / 1271558\n",
      "  processed 216000 / 1271558\n",
      "  processed 218000 / 1271558\n",
      "  processed 220000 / 1271558\n",
      "  processed 222000 / 1271558\n",
      "  processed 224000 / 1271558\n",
      "  processed 226000 / 1271558\n",
      "  processed 228000 / 1271558\n",
      "  processed 230000 / 1271558\n",
      "  processed 232000 / 1271558\n",
      "  processed 234000 / 1271558\n",
      "  processed 236000 / 1271558\n",
      "  processed 238000 / 1271558\n",
      "  processed 240000 / 1271558\n",
      "  processed 242000 / 1271558\n",
      "  processed 244000 / 1271558\n",
      "  processed 246000 / 1271558\n",
      "  processed 248000 / 1271558\n",
      "  processed 250000 / 1271558\n",
      "  processed 252000 / 1271558\n",
      "  processed 254000 / 1271558\n",
      "  processed 256000 / 1271558\n",
      "  processed 258000 / 1271558\n",
      "  processed 260000 / 1271558\n",
      "  processed 262000 / 1271558\n",
      "  processed 264000 / 1271558\n",
      "  processed 266000 / 1271558\n",
      "  processed 268000 / 1271558\n",
      "  processed 270000 / 1271558\n",
      "  processed 272000 / 1271558\n",
      "  processed 274000 / 1271558\n",
      "  processed 276000 / 1271558\n",
      "  processed 278000 / 1271558\n",
      "  processed 280000 / 1271558\n",
      "  processed 282000 / 1271558\n",
      "  processed 284000 / 1271558\n",
      "  processed 286000 / 1271558\n",
      "  processed 288000 / 1271558\n",
      "  processed 290000 / 1271558\n",
      "  processed 292000 / 1271558\n",
      "  processed 294000 / 1271558\n",
      "  processed 296000 / 1271558\n",
      "  processed 298000 / 1271558\n",
      "  processed 300000 / 1271558\n",
      "  processed 302000 / 1271558\n",
      "  processed 304000 / 1271558\n",
      "  processed 306000 / 1271558\n",
      "  processed 308000 / 1271558\n",
      "  processed 310000 / 1271558\n",
      "  processed 312000 / 1271558\n",
      "  processed 314000 / 1271558\n",
      "  processed 316000 / 1271558\n",
      "  processed 318000 / 1271558\n",
      "  processed 320000 / 1271558\n",
      "  processed 322000 / 1271558\n",
      "  processed 324000 / 1271558\n",
      "  processed 326000 / 1271558\n",
      "  processed 328000 / 1271558\n",
      "  processed 330000 / 1271558\n",
      "  processed 332000 / 1271558\n",
      "  processed 334000 / 1271558\n",
      "  processed 336000 / 1271558\n",
      "  processed 338000 / 1271558\n",
      "  processed 340000 / 1271558\n",
      "  processed 342000 / 1271558\n",
      "  processed 344000 / 1271558\n",
      "  processed 346000 / 1271558\n",
      "  processed 348000 / 1271558\n",
      "  processed 350000 / 1271558\n",
      "  processed 352000 / 1271558\n",
      "  processed 354000 / 1271558\n",
      "  processed 356000 / 1271558\n",
      "  processed 358000 / 1271558\n",
      "  processed 360000 / 1271558\n",
      "  processed 362000 / 1271558\n",
      "  processed 364000 / 1271558\n",
      "  processed 366000 / 1271558\n",
      "  processed 368000 / 1271558\n",
      "  processed 370000 / 1271558\n",
      "  processed 372000 / 1271558\n",
      "  processed 374000 / 1271558\n",
      "  processed 376000 / 1271558\n",
      "  processed 378000 / 1271558\n",
      "  processed 380000 / 1271558\n",
      "  processed 382000 / 1271558\n",
      "  processed 384000 / 1271558\n",
      "  processed 386000 / 1271558\n",
      "  processed 388000 / 1271558\n",
      "  processed 390000 / 1271558\n",
      "  processed 392000 / 1271558\n",
      "  processed 394000 / 1271558\n",
      "  processed 396000 / 1271558\n",
      "  processed 398000 / 1271558\n",
      "  processed 400000 / 1271558\n",
      "  processed 402000 / 1271558\n",
      "  processed 404000 / 1271558\n",
      "  processed 406000 / 1271558\n",
      "  processed 408000 / 1271558\n",
      "  processed 410000 / 1271558\n",
      "  processed 412000 / 1271558\n",
      "  processed 414000 / 1271558\n",
      "  processed 416000 / 1271558\n",
      "  processed 418000 / 1271558\n",
      "  processed 420000 / 1271558\n",
      "  processed 422000 / 1271558\n",
      "  processed 424000 / 1271558\n",
      "  processed 426000 / 1271558\n",
      "  processed 428000 / 1271558\n",
      "  processed 430000 / 1271558\n",
      "  processed 432000 / 1271558\n",
      "  processed 434000 / 1271558\n",
      "  processed 436000 / 1271558\n",
      "  processed 438000 / 1271558\n",
      "  processed 440000 / 1271558\n",
      "  processed 442000 / 1271558\n",
      "  processed 444000 / 1271558\n",
      "  processed 446000 / 1271558\n",
      "  processed 448000 / 1271558\n",
      "  processed 450000 / 1271558\n",
      "  processed 452000 / 1271558\n",
      "  processed 454000 / 1271558\n",
      "  processed 456000 / 1271558\n",
      "  processed 458000 / 1271558\n",
      "  processed 460000 / 1271558\n",
      "  processed 462000 / 1271558\n",
      "  processed 464000 / 1271558\n",
      "  processed 466000 / 1271558\n",
      "  processed 468000 / 1271558\n",
      "  processed 470000 / 1271558\n",
      "  processed 472000 / 1271558\n",
      "  processed 474000 / 1271558\n",
      "  processed 476000 / 1271558\n",
      "  processed 478000 / 1271558\n",
      "  processed 480000 / 1271558\n",
      "  processed 482000 / 1271558\n",
      "  processed 484000 / 1271558\n",
      "  processed 486000 / 1271558\n",
      "  processed 488000 / 1271558\n",
      "  processed 490000 / 1271558\n",
      "  processed 492000 / 1271558\n",
      "  processed 494000 / 1271558\n",
      "  processed 496000 / 1271558\n",
      "  processed 498000 / 1271558\n",
      "  processed 500000 / 1271558\n",
      "  processed 502000 / 1271558\n",
      "  processed 504000 / 1271558\n",
      "  processed 506000 / 1271558\n",
      "  processed 508000 / 1271558\n",
      "  processed 510000 / 1271558\n",
      "  processed 512000 / 1271558\n",
      "  processed 514000 / 1271558\n",
      "  processed 516000 / 1271558\n",
      "  processed 518000 / 1271558\n",
      "  processed 520000 / 1271558\n",
      "  processed 522000 / 1271558\n",
      "  processed 524000 / 1271558\n",
      "  processed 526000 / 1271558\n",
      "  processed 528000 / 1271558\n",
      "  processed 530000 / 1271558\n",
      "  processed 532000 / 1271558\n",
      "  processed 534000 / 1271558\n",
      "  processed 536000 / 1271558\n",
      "  processed 538000 / 1271558\n",
      "  processed 540000 / 1271558\n",
      "  processed 542000 / 1271558\n",
      "  processed 544000 / 1271558\n",
      "  processed 546000 / 1271558\n",
      "  processed 548000 / 1271558\n",
      "  processed 550000 / 1271558\n",
      "  processed 552000 / 1271558\n",
      "  processed 554000 / 1271558\n",
      "  processed 556000 / 1271558\n",
      "  processed 558000 / 1271558\n",
      "  processed 560000 / 1271558\n",
      "  processed 562000 / 1271558\n",
      "  processed 564000 / 1271558\n",
      "  processed 566000 / 1271558\n",
      "  processed 568000 / 1271558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  processed 570000 / 1271558\n",
      "  processed 572000 / 1271558\n",
      "  processed 574000 / 1271558\n",
      "  processed 576000 / 1271558\n",
      "  processed 578000 / 1271558\n",
      "  processed 580000 / 1271558\n",
      "  processed 582000 / 1271558\n",
      "  processed 584000 / 1271558\n",
      "  processed 586000 / 1271558\n",
      "  processed 588000 / 1271558\n",
      "  processed 590000 / 1271558\n",
      "  processed 592000 / 1271558\n",
      "  processed 594000 / 1271558\n",
      "  processed 596000 / 1271558\n",
      "  processed 598000 / 1271558\n",
      "  processed 600000 / 1271558\n",
      "  processed 602000 / 1271558\n",
      "  processed 604000 / 1271558\n",
      "  processed 606000 / 1271558\n",
      "  processed 608000 / 1271558\n",
      "  processed 610000 / 1271558\n",
      "  processed 612000 / 1271558\n",
      "  processed 614000 / 1271558\n",
      "  processed 616000 / 1271558\n",
      "  processed 618000 / 1271558\n",
      "  processed 620000 / 1271558\n",
      "  processed 622000 / 1271558\n",
      "  processed 624000 / 1271558\n",
      "  processed 626000 / 1271558\n",
      "  processed 628000 / 1271558\n",
      "  processed 630000 / 1271558\n",
      "  processed 632000 / 1271558\n",
      "  processed 634000 / 1271558\n",
      "  processed 636000 / 1271558\n",
      "  processed 638000 / 1271558\n",
      "  processed 640000 / 1271558\n",
      "  processed 642000 / 1271558\n",
      "  processed 644000 / 1271558\n",
      "  processed 646000 / 1271558\n",
      "  processed 648000 / 1271558\n",
      "  processed 650000 / 1271558\n",
      "  processed 652000 / 1271558\n",
      "  processed 654000 / 1271558\n",
      "  processed 656000 / 1271558\n",
      "  processed 658000 / 1271558\n",
      "  processed 660000 / 1271558\n",
      "  processed 662000 / 1271558\n",
      "  processed 664000 / 1271558\n",
      "  processed 666000 / 1271558\n",
      "  processed 668000 / 1271558\n",
      "  processed 670000 / 1271558\n",
      "  processed 672000 / 1271558\n",
      "  processed 674000 / 1271558\n",
      "  processed 676000 / 1271558\n",
      "  processed 678000 / 1271558\n",
      "  processed 680000 / 1271558\n",
      "  processed 682000 / 1271558\n",
      "  processed 684000 / 1271558\n",
      "  processed 686000 / 1271558\n",
      "  processed 688000 / 1271558\n",
      "  processed 690000 / 1271558\n",
      "  processed 692000 / 1271558\n",
      "  processed 694000 / 1271558\n",
      "  processed 696000 / 1271558\n",
      "  processed 698000 / 1271558\n",
      "  processed 700000 / 1271558\n",
      "  processed 702000 / 1271558\n",
      "  processed 704000 / 1271558\n",
      "  processed 706000 / 1271558\n",
      "  processed 708000 / 1271558\n",
      "  processed 710000 / 1271558\n",
      "  processed 712000 / 1271558\n",
      "  processed 714000 / 1271558\n",
      "  processed 716000 / 1271558\n",
      "  processed 718000 / 1271558\n",
      "  processed 720000 / 1271558\n",
      "  processed 722000 / 1271558\n",
      "  processed 724000 / 1271558\n",
      "  processed 726000 / 1271558\n",
      "  processed 728000 / 1271558\n",
      "  processed 730000 / 1271558\n",
      "  processed 732000 / 1271558\n",
      "  processed 734000 / 1271558\n",
      "  processed 736000 / 1271558\n",
      "  processed 738000 / 1271558\n",
      "  processed 740000 / 1271558\n",
      "  processed 742000 / 1271558\n",
      "  processed 744000 / 1271558\n",
      "  processed 746000 / 1271558\n",
      "  processed 748000 / 1271558\n",
      "  processed 750000 / 1271558\n",
      "  processed 752000 / 1271558\n",
      "  processed 754000 / 1271558\n",
      "  processed 756000 / 1271558\n",
      "  processed 758000 / 1271558\n",
      "  processed 760000 / 1271558\n",
      "  processed 762000 / 1271558\n",
      "  processed 764000 / 1271558\n",
      "  processed 766000 / 1271558\n",
      "  processed 768000 / 1271558\n",
      "  processed 770000 / 1271558\n",
      "  processed 772000 / 1271558\n",
      "  processed 774000 / 1271558\n",
      "  processed 776000 / 1271558\n",
      "  processed 778000 / 1271558\n",
      "  processed 780000 / 1271558\n",
      "  processed 782000 / 1271558\n",
      "  processed 784000 / 1271558\n",
      "  processed 786000 / 1271558\n",
      "  processed 788000 / 1271558\n",
      "  processed 790000 / 1271558\n",
      "  processed 792000 / 1271558\n",
      "  processed 794000 / 1271558\n",
      "  processed 796000 / 1271558\n",
      "  processed 798000 / 1271558\n",
      "  processed 800000 / 1271558\n",
      "  processed 802000 / 1271558\n",
      "  processed 804000 / 1271558\n",
      "  processed 806000 / 1271558\n",
      "  processed 808000 / 1271558\n",
      "  processed 810000 / 1271558\n",
      "  processed 812000 / 1271558\n",
      "  processed 814000 / 1271558\n",
      "  processed 816000 / 1271558\n",
      "  processed 818000 / 1271558\n",
      "  processed 820000 / 1271558\n",
      "  processed 822000 / 1271558\n",
      "  processed 824000 / 1271558\n",
      "  processed 826000 / 1271558\n",
      "  processed 828000 / 1271558\n",
      "  processed 830000 / 1271558\n",
      "  processed 832000 / 1271558\n",
      "  processed 834000 / 1271558\n",
      "  processed 836000 / 1271558\n",
      "  processed 838000 / 1271558\n",
      "  processed 840000 / 1271558\n",
      "  processed 842000 / 1271558\n",
      "  processed 844000 / 1271558\n",
      "  processed 846000 / 1271558\n",
      "  processed 848000 / 1271558\n",
      "  processed 850000 / 1271558\n",
      "  processed 852000 / 1271558\n",
      "  processed 854000 / 1271558\n",
      "  processed 856000 / 1271558\n",
      "  processed 858000 / 1271558\n",
      "  processed 860000 / 1271558\n",
      "  processed 862000 / 1271558\n",
      "  processed 864000 / 1271558\n",
      "  processed 866000 / 1271558\n",
      "  processed 868000 / 1271558\n",
      "  processed 870000 / 1271558\n",
      "  processed 872000 / 1271558\n"
     ]
    }
   ],
   "source": [
    "word2idx,W1,W2=train(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    with open('./model/word2vec.json','r') as f:\n",
    "        word2idx=json.load(f)\n",
    "    weights = np.load('./model/weights.npz')\n",
    "    W1=weights['arr_0']\n",
    "    W2=weights['arr_1']\n",
    "    return word2idx,W1,W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_analogies_GloVe(w1,w2,w3):\n",
    "    for w in (w1, w2, w3):\n",
    "        if w not in word2vec:\n",
    "            print(\"{} not in word2vec\".format(w))\n",
    "\n",
    "    king = word2vec[w1]\n",
    "    man = word2vec[w2]\n",
    "    woman = word2vec[w3]\n",
    "    v0 = king-man+woman\n",
    "    distances = pairwise_distances(v0.reshape(1,D), embedding, metric='cosine')\n",
    "    distances = distances.reshape(V)\n",
    "    idxs = distances.argsort()[:4]\n",
    "    for idx in idxs:\n",
    "        word = idx2word[idx]\n",
    "        if  word not in (w1,w2,w3):\n",
    "            best_word = word\n",
    "            break\n",
    "    print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)\n",
    "\n",
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word,  word_embedding):\n",
    "    V,D=word_embedding.shape\n",
    "    for w in (pos1, neg1, pos2, neg2):\n",
    "        if w not in word2idx:\n",
    "            print(w,\" not in the dictionary\")\n",
    "            return\n",
    "\n",
    "    p1=word_embedding[word2idx[pos1]]\n",
    "    n1=word_embedding[word2idx[neg1]]\n",
    "    p2=word_embedding[word2idx[pos2]]\n",
    "    n2=word_embedding[word2idx[neg2]]\n",
    "\n",
    "    vec=p1-n1+n2\n",
    "    distances = pairwise_distances(vec.reshape((1,D)),word_embedding,metric='cosine').reshape(V)\n",
    "\n",
    "    idx=distances.argsort()[:10]\n",
    "    \n",
    "    for i in idx:\n",
    "        word= idx2word[i]\n",
    "        if word not in (pos1, neg1, neg2):\n",
    "            best_word=word\n",
    "            break\n",
    "    print(\"got: %s - %s = %s - %s\" % (pos1, neg1, best_word, neg2))\n",
    "    print(\"closest 10:\")\n",
    "    for i in idx:\n",
    "        print(idx2word[i], distances[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "got: king - man = throne - woman\n",
      "closest 10:\n",
      "king 0.14414545237811727\n",
      "throne 0.24656253580136755\n",
      "son 0.267252433482311\n",
      "emperor 0.26938471601024083\n",
      "refused 0.27929521222287734\n",
      "duke 0.28422610458449316\n",
      "kingdom 0.28608770960783136\n",
      "kings 0.2870797133460291\n",
      "death 0.28884894888982726\n",
      "empire 0.2932562292513611\n",
      "\n",
      "got: king - prince = reign - princess\n",
      "closest 10:\n",
      "princess 0.15603193879218136\n",
      "king 0.303246093978617\n",
      "reign 0.35603959915712147\n",
      "emperor 0.3677059694634932\n",
      "rome 0.3842310894522529\n",
      "church 0.38556999550249393\n",
      "kings 0.386555663253876\n",
      "throne 0.3902889021363075\n",
      "duchy 0.392025662558692\n",
      "holy 0.39738539408202\n",
      "\n",
      "got: miami - florida = thirteenth - texas\n",
      "closest 10:\n",
      "miami 0.03146014966786037\n",
      "thirteenth 0.48799562373487304\n",
      "illustrations 0.4976661846445509\n",
      "persecution 0.5245347552419982\n",
      "embedded 0.5257009224935012\n",
      "hierarchy 0.526959423369191\n",
      "humor 0.5277639982051912\n",
      "expects 0.5293928472867034\n",
      "fbi 0.5326471935564904\n",
      "remember 0.5382997996050276\n",
      "\n",
      "picasso  not in the dictionary\n",
      "sushi  not in the dictionary\n",
      "got: man - woman = career - she\n",
      "closest 10:\n",
      "she 0.14956514568876755\n",
      "man 0.22013602597014137\n",
      "career 0.25717513520542923\n",
      "robert 0.27446917192320286\n",
      "writer 0.30375170372562044\n",
      "wrote 0.30488133139663876\n",
      "her 0.31092035322369693\n",
      "prince 0.3225577324359179\n",
      "whom 0.32496113902373014\n",
      "mother 0.33287504909231147\n",
      "\n",
      "got: man - woman = 285 - aunt\n",
      "closest 10:\n",
      "aunt 0.014947980164934482\n",
      "285 0.44118192839555825\n",
      "treasury 0.4479504674013617\n",
      "monarchs 0.4713547777074246\n",
      "gave 0.5002845282772959\n",
      "died 0.5102447703334481\n",
      "reconstructed 0.5149048082548788\n",
      "1243 0.5216977220821879\n",
      "mary 0.5225395027653522\n",
      "ha 0.5234297357512205\n",
      "\n",
      "got: man - woman = italian - sister\n",
      "closest 10:\n",
      "sister 0.20909690530227087\n",
      "italian 0.3425076676645723\n",
      "governor 0.34264557642664817\n",
      "von 0.3748514315397894\n",
      "prominent 0.3851610875238516\n",
      "tradition 0.4066899611136645\n",
      "son 0.410849108982939\n",
      "negotiations 0.41135911763221145\n",
      "appointed 0.4152514594697563\n",
      "ruled 0.42541787725770985\n",
      "\n",
      "got: man - woman = she - wife\n",
      "closest 10:\n",
      "wife 0.15350468781143634\n",
      "she 0.25822259012336934\n",
      "lord 0.2649753734055361\n",
      "frederick 0.26861833991033\n",
      "mother 0.27458081081685237\n",
      "met 0.2762201403784251\n",
      "prince 0.2784599760141039\n",
      "born 0.281711359559222\n",
      "her 0.2828638492526061\n",
      "marriage 0.2914401306671901\n",
      "\n",
      "got: man - woman = actor - actress\n",
      "closest 10:\n",
      "actress 0.07286382731843788\n",
      "actor 0.284368478554769\n",
      "film 0.2875956059473921\n",
      "best 0.29705907536478104\n",
      "singer 0.30662497430907076\n",
      "films 0.3066369484813263\n",
      "played 0.3171447006691407\n",
      "career 0.3226229184976639\n",
      "starred 0.3411377236715514\n",
      "drama 0.3420386261043403\n",
      "\n",
      "got: man - woman = robert - mother\n",
      "closest 10:\n",
      "mother 0.1511196207042177\n",
      "man 0.24144387225858355\n",
      "robert 0.30661669803412184\n",
      "she 0.308851539748889\n",
      "whom 0.32320803680574806\n",
      "born 0.3377218578656501\n",
      "i 0.34459059055944874\n",
      "her 0.3459468076348644\n",
      "irish 0.34857556119498856\n",
      "career 0.3517724914803074\n",
      "\n",
      "got: heir - heiress = veil - princess\n",
      "closest 10:\n",
      "veil 0.440059910481012\n",
      "already 0.45128116020853104\n",
      "rome 0.45684015080679774\n",
      "iv 0.4657299046570337\n",
      "vi 0.47840685209145595\n",
      "bishop 0.4972451607440227\n",
      "marriage 0.4980565814891167\n",
      "sweden 0.5031406214400442\n",
      "chroniclers 0.5216855764356861\n",
      "1381 0.5226938135293462\n",
      "\n",
      "got: nephew - niece = egypt - aunt\n",
      "closest 10:\n",
      "aunt 0.3064213904115112\n",
      "egypt 0.4817446774119538\n",
      "priests 0.49225360161583886\n",
      "commodity 0.5058087443466346\n",
      "manpower 0.520899857240473\n",
      "nephew 0.5271302184245469\n",
      "letter 0.5286221740818073\n",
      "caught 0.5322058078536972\n",
      "hamburger 0.5413850309476566\n",
      "journalist 0.5423927737027646\n",
      "\n",
      "got: france - paris = federico - tokyo\n",
      "closest 10:\n",
      "tokyo 0.08186685867609755\n",
      "federico 0.41973289670892344\n",
      "austrian 0.4407981946443864\n",
      "powers 0.4843498335564561\n",
      "remained 0.4948245856675151\n",
      "coined 0.4976644642893415\n",
      "honour 0.5094781278910725\n",
      "holy 0.5095133473125815\n",
      "politics 0.5127516687037733\n",
      "retired 0.5152700227895817\n",
      "\n",
      "got: france - paris = repaid - beijing\n",
      "closest 10:\n",
      "beijing 0.045313162955062714\n",
      "repaid 0.46663421867111343\n",
      "canada 0.47461258009630725\n",
      "special 0.49556247977392465\n",
      "canine 0.4977383223483147\n",
      "architects 0.5041672946100242\n",
      "service 0.5092746437822042\n",
      "translations 0.5168864864633582\n",
      "compete 0.5251299990770758\n",
      "overthrow 0.5263965801591672\n",
      "\n",
      "got: february - january = december - november\n",
      "closest 10:\n",
      "november 0.12197697402538943\n",
      "february 0.18516143980022504\n",
      "december 0.266133467521221\n",
      "took 0.28577176112283353\n",
      "entered 0.30172771671690124\n",
      "21 0.3062261262273017\n",
      "founded 0.306751733802335\n",
      "captured 0.3162596097343343\n",
      "netherlands 0.317200523563852\n",
      "football 0.3181363295008086\n",
      "\n",
      "got: france - paris = peace - berlin\n",
      "closest 10:\n",
      "berlin 0.2278672016348653\n",
      "france 0.28421919043328747\n",
      "peace 0.2913791979858018\n",
      "council 0.31954864153532214\n",
      "appointed 0.32788414733315796\n",
      "independence 0.3403030875351841\n",
      "elected 0.3407070253958382\n",
      "february 0.3425473354363816\n",
      "constitution 0.34556765907348386\n",
      "germany 0.34946188936230416\n",
      "\n",
      "got: week - day = win - month\n",
      "closest 10:\n",
      "month 0.27916815851452925\n",
      "week 0.34824316290589363\n",
      "win 0.4354520383792414\n",
      "record 0.4446982492324798\n",
      "ran 0.4503683020262277\n",
      "reached 0.49018225528808146\n",
      "playoffs 0.4978266192241184\n",
      "track 0.5097682674025318\n",
      "annual 0.5105345723132984\n",
      "project 0.5211314170297587\n",
      "\n",
      "got: week - day = hot - minute\n",
      "closest 10:\n",
      "minute 0.06028502630894761\n",
      "hot 0.46017462424625444\n",
      "single 0.46570876588236654\n",
      "mechanism 0.46616392172362386\n",
      "producing 0.5017905134503483\n",
      "playoffs 0.502821832020697\n",
      "player 0.5093696018188052\n",
      "shows 0.512015466953496\n",
      "healthy 0.523242560633717\n",
      "diminished 0.5254233456432368\n",
      "\n",
      "got: france - paris = empire - rome\n",
      "closest 10:\n",
      "rome 0.1894345109737371\n",
      "empire 0.20976752290306366\n",
      "france 0.21455899459152727\n",
      "king 0.27625377872354506\n",
      "emperor 0.29033220933858095\n",
      "kingdom 0.2925760088482542\n",
      "rule 0.2964036919557572\n",
      "alexander 0.30009577467002757\n",
      "reign 0.30801551089325563\n",
      "governor 0.30840479887186156\n",
      "\n",
      "got: paris - france = capital - italy\n",
      "closest 10:\n",
      "italy 0.2513197708960876\n",
      "paris 0.3399884399498896\n",
      "capital 0.39577043739034257\n",
      "residence 0.44348342502725346\n",
      "fort 0.4533082672639658\n",
      "exile 0.45508134061090755\n",
      "attack 0.4590227403496181\n",
      "arrived 0.46246858610884656\n",
      "cathedral 0.464064723509762\n",
      "participated 0.4677471739406308\n",
      "\n",
      "got: france - french = births - english\n",
      "closest 10:\n",
      "english 0.15314589026973446\n",
      "births 0.30271677144147335\n",
      "writers 0.3229686590178056\n",
      "history 0.3458969387222114\n",
      "him 0.3669371997975067\n",
      "deaths 0.37019862016862826\n",
      "born 0.3781469899378853\n",
      "who 0.3879150369103115\n",
      "france 0.3888466544229501\n",
      "was 0.38947269408599183\n",
      "\n",
      "got: japan - japanese = ɡ - chinese\n",
      "closest 10:\n",
      "chinese 0.330490279691654\n",
      "japan 0.3672981881926175\n",
      "ɡ 0.5089202523063544\n",
      "twentyone 0.5154444273124454\n",
      "madeira 0.5197836042043407\n",
      "asia 0.5232702659377282\n",
      "patricia 0.5257627422710145\n",
      "6th 0.5268416872978905\n",
      "storm 0.5291144874812688\n",
      "sloping 0.5414645852226244\n",
      "\n",
      "got: china - chinese = 2001 - american\n",
      "closest 10:\n",
      "american 0.20002848222097924\n",
      "2001 0.3286285935443617\n",
      "america 0.32980862883848683\n",
      "francisco 0.34210443448198535\n",
      "2007 0.34364567367984555\n",
      "park 0.3437214811087397\n",
      "2018 0.3454244605626119\n",
      "los 0.36110368764994527\n",
      "actors 0.36440797308890827\n",
      "2010 0.36686958539858516\n",
      "\n",
      "got: japan - japanese = edward - italian\n",
      "closest 10:\n",
      "italian 0.23884150328529907\n",
      "edward 0.3325644430241206\n",
      "naples 0.3651687462258736\n",
      "england 0.36583026210420444\n",
      "historian 0.3857125402250762\n",
      "succeeded 0.3916103727624426\n",
      "commissioned 0.4025653167110459\n",
      "prominent 0.4135696798561306\n",
      "son 0.41866982133853736\n",
      "wars 0.42470874171154727\n",
      "\n",
      "got: japan - japanese = shoulder - australian\n",
      "closest 10:\n",
      "australian 0.23803152464389554\n",
      "shoulder 0.42314846356202873\n",
      "campuses 0.4573012667729981\n",
      "clubs 0.4605969629586961\n",
      "overly 0.4919466330593312\n",
      "popular 0.49743695404927746\n",
      "fc 0.4979068757761934\n",
      "acronym 0.5047101200164852\n",
      "team 0.5118361536076592\n",
      "teams 0.5126215594969566\n",
      "\n",
      "got: walk - walking = eddie - swimming\n",
      "closest 10:\n",
      "swimming 0.2128173361771204\n",
      "eddie 0.4513694310340154\n",
      "walk 0.45394684238487537\n",
      "finland 0.5280047748047353\n",
      "advisory 0.5286913077854223\n",
      "filming 0.5310676193367834\n",
      "playing 0.5366179195292871\n",
      "congo 0.5471506053770618\n",
      "sullivan 0.5482359818000532\n",
      "merger 0.5485906287334372\n",
      "\n",
      "**********\n",
      "got: king - man = throne - woman\n",
      "closest 10:\n",
      "king 0.10179374930667529\n",
      "throne 0.1702838595361471\n",
      "son 0.21298176095787813\n",
      "reign 0.21313411432626606\n",
      "pope 0.21917431029802437\n",
      "kings 0.22936867868363986\n",
      "frederick 0.23379418719555833\n",
      "philip 0.2381860601856962\n",
      "brother 0.2527665557930079\n",
      "edward 0.256087373859301\n",
      "\n",
      "got: king - prince = throne - princess\n",
      "closest 10:\n",
      "princess 0.1487337833691882\n",
      "king 0.2396997584224969\n",
      "throne 0.31187874476744537\n",
      "succeeded 0.32352692132723027\n",
      "duchy 0.3340565313042977\n",
      "reign 0.3346209006517237\n",
      "kingdom 0.33478836483927654\n",
      "roman 0.33832442036387445\n",
      "kingdoms 0.3419148926412411\n",
      "duke 0.34704064865218676\n",
      "\n",
      "got: miami - florida = burn - texas\n",
      "closest 10:\n",
      "miami 0.04938423066668174\n",
      "burn 0.45979998019619417\n",
      "hypothesized 0.47798540200113315\n",
      "liberia 0.5107681269989752\n",
      "investing 0.5228568689849498\n",
      "stateoftheart 0.5266659944861821\n",
      "nlf 0.5354067798205924\n",
      "lakoff 0.5432924593909452\n",
      "possibilities 0.546950109206888\n",
      "grasp 0.5479569117556821\n",
      "\n",
      "picasso  not in the dictionary\n",
      "sushi  not in the dictionary\n",
      "got: man - woman = her - she\n",
      "closest 10:\n",
      "she 0.12833803167253133\n",
      "man 0.18759204765791093\n",
      "her 0.2512140446758975\n",
      "prince 0.2519342716066322\n",
      "writer 0.25454859311689915\n",
      "career 0.2554766796124286\n",
      "wrote 0.2614738197120816\n",
      "played 0.2684676239523437\n",
      "actor 0.274658097414627\n",
      "robert 0.27769789098857345\n",
      "\n",
      "got: man - woman = died - aunt\n",
      "closest 10:\n",
      "aunt 0.020409972740382254\n",
      "died 0.46735967080671315\n",
      "dynasty 0.46827701491549223\n",
      "285 0.49330090477453215\n",
      "death 0.49621696941072924\n",
      "wife 0.49700519132378584\n",
      "cousin 0.502637403817686\n",
      "his 0.5202682502779769\n",
      "mother 0.5211381844940817\n",
      "son 0.5240231932116433\n",
      "\n",
      "got: man - woman = charles - sister\n",
      "closest 10:\n",
      "sister 0.16453132481648125\n",
      "charles 0.28709149074705276\n",
      "brother 0.29912648850879775\n",
      "george 0.3015761858547199\n",
      "joseph 0.3048272239684866\n",
      "queen 0.30706837051541847\n",
      "daughter 0.3072950128589712\n",
      "himself 0.31373606408392374\n",
      "son 0.31503278696208636\n",
      "appointed 0.3198051239942227\n",
      "\n",
      "got: man - woman = she - wife\n",
      "closest 10:\n",
      "wife 0.13066489470348275\n",
      "she 0.16993416644025994\n",
      "prince 0.17033051974350866\n",
      "daughter 0.19492219040769965\n",
      "born 0.2016469239615496\n",
      "historian 0.20733543725283343\n",
      "queen 0.21570065872242694\n",
      "father 0.21861598492449974\n",
      "his 0.22159633322290462\n",
      "met 0.22253230759701992\n",
      "\n",
      "got: man - woman = final - actress\n",
      "closest 10:\n",
      "actress 0.05063943176637897\n",
      "final 0.23762568268774065\n",
      "film 0.24995170589954763\n",
      "actor 0.251161845495137\n",
      "best 0.2631548791167635\n",
      "played 0.2644214386253342\n",
      "movie 0.2677158156950584\n",
      "career 0.27218179205375204\n",
      "appeared 0.27373383034744025\n",
      "films 0.27913318599051684\n",
      "\n",
      "got: man - woman = she - mother\n",
      "closest 10:\n",
      "mother 0.14037744716230027\n",
      "man 0.21477773226862884\n",
      "she 0.2277794702606284\n",
      "his 0.2369008642342194\n",
      "i 0.25153342413480484\n",
      "daughter 0.25809389983657005\n",
      "prince 0.26389601974217625\n",
      "later 0.26559388030679554\n",
      "born 0.2671497163893838\n",
      "father 0.2721155942442717\n",
      "\n",
      "got: heir - heiress = tsar - princess\n",
      "closest 10:\n",
      "tsar 0.46087890473641346\n",
      "prix 0.4753826687270052\n",
      "austrian 0.4785369399265218\n",
      "accept 0.48324050768943083\n",
      "heir 0.4867236032294573\n",
      "vi 0.4870954807439539\n",
      "who 0.4963105052972683\n",
      "ferdinand 0.49821978931306754\n",
      "son 0.5016843395626177\n",
      "wilhelm 0.5040799310011326\n",
      "\n",
      "got: nephew - niece = priest - aunt\n",
      "closest 10:\n",
      "aunt 0.28227510531465394\n",
      "priest 0.42053672717281676\n",
      "nephew 0.4533792226195358\n",
      "influenced 0.4611972961238625\n",
      "lombard 0.4807827339074535\n",
      "zeus 0.4984413629840415\n",
      "egypt 0.5049115083469734\n",
      "apollo 0.5070605660383138\n",
      "deities 0.5078332448009423\n",
      "jews 0.5129624405684704\n",
      "\n",
      "got: france - paris = succeeded - tokyo\n",
      "closest 10:\n",
      "tokyo 0.10291301560320532\n",
      "succeeded 0.4810845944527249\n",
      "reigning 0.5054068400184129\n",
      "jury 0.5057039761913666\n",
      "council 0.509049083764632\n",
      "house 0.5131856529202036\n",
      "official 0.5175478613964979\n",
      "brandt 0.5185822177085999\n",
      "harald 0.5211996720759464\n",
      "ayrshire 0.5269643210288758\n",
      "\n",
      "got: france - paris = cruise - beijing\n",
      "closest 10:\n",
      "beijing 0.023839735103496462\n",
      "cruise 0.4850752126853487\n",
      "architects 0.49266157108197284\n",
      "arrested 0.5005294454670881\n",
      "bangladesh 0.5021408590783465\n",
      "canyon 0.5100945978235474\n",
      "office 0.5163282139999725\n",
      "documents 0.5169966858554842\n",
      "paramount 0.5201451286245229\n",
      "cantons 0.5242963032365164\n",
      "\n",
      "got: february - january = entered - november\n",
      "closest 10:\n",
      "november 0.10213215350138793\n",
      "february 0.11412417775418093\n",
      "entered 0.18205138089747608\n",
      "december 0.19534921578000197\n",
      "october 0.21181973480521532\n",
      "joined 0.21400757173079765\n",
      "16 0.21688843068569796\n",
      "september 0.22262952727940277\n",
      "took 0.22396140404214682\n",
      "grand 0.23299839571742054\n",
      "\n",
      "got: france - paris = council - berlin\n",
      "closest 10:\n",
      "berlin 0.14747968490285435\n",
      "council 0.2544676654612248\n",
      "germany 0.27773166963758056\n",
      "elected 0.28073773814365344\n",
      "appointed 0.28547365821838855\n",
      "france 0.300423401842399\n",
      "following 0.3068472665818278\n",
      "washington 0.3091306601551661\n",
      "constitution 0.31383430803669454\n",
      "peace 0.31808322319899696\n",
      "\n",
      "got: week - day = kensington - month\n",
      "closest 10:\n",
      "month 0.13695526470416908\n",
      "week 0.3656446886560216\n",
      "kensington 0.3815100873060606\n",
      "christmas 0.4481286837963241\n",
      "record 0.45017615500819386\n",
      "getting 0.45129894453422137\n",
      "100 0.45819947761265933\n",
      "greatest 0.46629037009363894\n",
      "days 0.4670067043078623\n",
      "featured 0.47650758959048767\n",
      "\n",
      "got: week - day = hot - minute\n",
      "closest 10:\n",
      "minute 0.02890009510614\n",
      "hot 0.44390357690676574\n",
      "soft 0.4449418255378388\n",
      "single 0.4645449365690272\n",
      "alcohol 0.47211566991968235\n",
      "asteroids 0.4743202149076776\n",
      "60 0.47537229081040455\n",
      "seeds 0.47555986402369277\n",
      "allows 0.4757116110237639\n",
      "mm 0.47606039036379166\n",
      "\n",
      "got: france - paris = empire - rome\n",
      "closest 10:\n",
      "rome 0.12265133095858805\n",
      "empire 0.1841342278357364\n",
      "kings 0.22555372872631696\n",
      "king 0.2270463157048035\n",
      "duchy 0.2275922267990409\n",
      "ruled 0.23654929662644641\n",
      "reign 0.23760960550898635\n",
      "kingdom 0.24018763153285028\n",
      "emperor 0.2473708284254693\n",
      "france 0.2503885957909384\n",
      "\n",
      "got: paris - france = leadership - italy\n",
      "closest 10:\n",
      "italy 0.2175417621994571\n",
      "paris 0.2292889199362963\n",
      "leadership 0.2881178336333201\n",
      "war 0.3154751495890151\n",
      "shortly 0.3164330645637271\n",
      "hill 0.33062745301715724\n",
      "fell 0.3339618416193365\n",
      "founded 0.34155410728646673\n",
      "philip 0.3417178724988186\n",
      "duke 0.3425540038666295\n",
      "\n",
      "got: france - french = writers - english\n",
      "closest 10:\n",
      "english 0.1588464203946739\n",
      "writers 0.22899637084781155\n",
      "deaths 0.31019342921133597\n",
      "births 0.3102070854984972\n",
      "born 0.3146540740025412\n",
      "descent 0.31557663756574716\n",
      "whom 0.34306856108200867\n",
      "mother 0.34940939265779747\n",
      "son 0.3579541035522421\n",
      "alumni 0.36190025897582345\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got: japan - japanese = athens - chinese\n",
      "closest 10:\n",
      "chinese 0.21953527885205648\n",
      "japan 0.2788977490014287\n",
      "athens 0.3280739820399725\n",
      "independence 0.3491686747849645\n",
      "soldiers 0.3510886814516735\n",
      "destroyed 0.3515397833248818\n",
      "occupation 0.35357179371364555\n",
      "colonial 0.3594536264729594\n",
      "serving 0.36548033588682216\n",
      "germany 0.36837307771440153\n",
      "\n",
      "got: china - chinese = television - american\n",
      "closest 10:\n",
      "american 0.14386885145887796\n",
      "television 0.2610146565708056\n",
      "park 0.2715182477531476\n",
      "actors 0.2798059081721269\n",
      "2013 0.28558477013483186\n",
      "america 0.29585443404035805\n",
      "2005 0.3097679121421274\n",
      "17 0.31432517672335114\n",
      "1993 0.3181790369467482\n",
      "directors 0.31960490582034284\n",
      "\n",
      "got: japan - japanese = france - italian\n",
      "closest 10:\n",
      "italian 0.1390341003255292\n",
      "france 0.244921477661169\n",
      "succeeded 0.2537441912663132\n",
      "defeat 0.2694627957496559\n",
      "chief 0.2736483227087031\n",
      "minister 0.2790252203531466\n",
      "wars 0.2799364218658662\n",
      "brother 0.2852730349752506\n",
      "queen 0.2859845664544576\n",
      "remained 0.28658925031589266\n",
      "\n",
      "got: japan - japanese = 2005 - australian\n",
      "closest 10:\n",
      "australian 0.15708305457097627\n",
      "2005 0.3699550854629241\n",
      "seven 0.37130943811010164\n",
      "stadium 0.37359473332715354\n",
      "2015 0.3774158990267833\n",
      "honorary 0.37823595617606065\n",
      "13 0.3840260769059878\n",
      "australia 0.3895679416798923\n",
      "2006 0.3998652068973052\n",
      "news 0.4011133219021036\n",
      "\n",
      "got: walk - walking = decade - swimming\n",
      "closest 10:\n",
      "swimming 0.21401910012488434\n",
      "walk 0.4605859965551662\n",
      "decade 0.4786153270950614\n",
      "haunted 0.4811693561794568\n",
      "fencing 0.489845022999721\n",
      "enemy 0.4915856935399525\n",
      "pigs 0.5217096134731343\n",
      "theater 0.5341541250282513\n",
      "circa 0.540396487144881\n",
      "tuc 0.5447027868965537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2idx,W1,W2=load_model()\n",
    "idx2word={v:k for k,v in word2idx.items()}\n",
    "\n",
    "for We in (W1, (W1 + W2.T) / 2):\n",
    "    print(\"**********\")\n",
    "\n",
    "    analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n",
    "    analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n",
    "    analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n",
    "    analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n",
    "    analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n",
    "    analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n",
    "    analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n",
    "    analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n",
    "    analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n",
    "    analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n",
    "    analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n",
    "    analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n",
    "    analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n",
    "    analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n",
    "    analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n",
    "    analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n",
    "    analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
