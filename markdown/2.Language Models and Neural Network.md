# Bigrams and Language Models

## What is a Bigram?

**Bigram**: 2 consecutive words in a sentence

Ex. 

> "The quick brown fox jumps over the lazy dog."

- Bigrams:
  - The quick
  - quick brown
  - brown fox
  - ...



**N-grams**: a sequence of N consecutive words





## The Bigram Model

$$
\text{Bigram model}: p(w_t|w_{t-1})
$$

- Ex. $p("brown" | "quick") = 0.5$, $p("the" | "the") = 0$
- How do we find these probs?

#### It's just counting!

- $p("brown" | "quick") = 0.5$

  - How many times does "quick " $\rightarrow$ "brown" appear in my set of documents?

  - How many times does "quick " appear in my set of documents?
  - Divide these 2!

  $$
  p(brown|quick)=\frac{\text{count}(quick \rightarrow brown)}{\text{count}(quick)}
  $$

#### What is a "set of documents"?

- We must have some training data - a list of example sentences - to create our model
  - Wikipedia
  - Scrape your own data form the internet
  - In the end: it's just a file or files containing sentences -> Corpus



## Language Model

- How to bigrams help us represent the prob of a sentence?

- Assumptions:

  - First, instead of "The quick brown fox jumps over the lazy dog.", let's use a simpler sentence: "A B C"
  - I want to find : $p("A B C")$

  **Bayes Rule**
  $$
  p(A \rightarrow B \rightarrow C)=p(C|A\rightarrow B)p(B|A)p(A)
  $$
  

##### Unigram

- Let's start with $p(A)$ and find $p(C|A\rightarrow B)$ next
  $$
  p(A)=\frac{\text{count}(A)}{\text{corpus length}}
  $$

##### Trigram

$$
p(C|A\rightarrow  B)=\frac{\text{count}(A \rightarrow B \rightarrow C)}{\text{count}(A \rightarrow B)}
$$

##### A longer sentence

$p(A \rightarrow B \rightarrow C  \rightarrow D \rightarrow E) =p(E| A \rightarrow B \rightarrow C  \rightarrow D)p(D|A \rightarrow B \rightarrow C  )p(C|A \rightarrow B )p(B|A)p(A)$



#### Add-one smoothing

- To overcome the zero probabilities 

- Instead of maximum-likelihood count, we add a small number to each count
- Note: $V$=vocabulary size=number of distinct words
- Divide by $V$ in denominator to ensure probs sum to $1$, which is necessary to form a valid prob distribution 

$$
p_{smooth}(B|A)=\frac{\text{count} (A \rightarrow{} B)+1}{\text{count}(A)+V}
$$

- Even if the sequence AB never occurs in our corpus we still get a tiny prob value for it which makes it at least possible 



#### The Markov Assumption

> What I see now depends ONLY on what I saw in the previous step

$$
p(w_t|w_{t-1},w_{t-2},...,w_{1})=p(w_t|w_{t-1})
$$

##### Example

$$
p(E|A,B,C,D)=p(E|D)
$$

### A model with only bigrams

- Perhaps the sentence "The quick brown fox jumps over the lazy **turtle**." never appears in our corpus
- But phrases like "lazy turtle" <u>probably do</u>.
- It's easier to model the prob of shorter phrases because we have more samples, and this makes a realistic sentence like the ones above more probable.

$$
p(A,B,C,D,E)=p(E|D)p(D|C)p(C|B)p(B|A)p(A)
$$

#### Hint

- Use **log-probabilities ** instead of probability
  - If a>b, then $\log(a)>\log{b}$
  - Probs are between 0 and 1.If you multiply too many of them, you might encounter the underflow problem, which means your answer gets rounded down to 0
- Normalize each sentence
  -  Log probs are always negative(<0)
  - The longer the sentence, the more -ve numbers we have to add together
  - Result: short sentences usually have higher log probs than longer sentences (always a bias towards short sentences)

$$
\frac{1}{T}\log p(w_1,...,w_T)=\frac{1}{T}[\log p(w_1)+\sum_{t=2}^T \log p(w_t|w_{t-1})]
$$



# Neural Bigram Model

### A neural model for bigrams

- Instead of counting, let's use a neuron to model $p(w_t|w_{t-1})$

- Recall: neuron == logistic regression



#### How do we represent words

$$
logit=w^Tx
$$

- $x$ must be a vector containing numbers

#### How can logistic regression be used?

- If input vector $x$ = last word, output vector $y$ = current word, then this is exactly a bigram!

$$
p(y|x)=softmax(W^Tx)
$$

#### All data is the same

- $X$: $N \times D$ matrix of input samples
- $Y$: $N \times  K$ matrix of output targets
  - $D$: # of input features
  - $K$: # of output classes
- In our case, we want $D=V, K=V$. Because 
  - input = one-hot encoded last word
  - output = one-hot encoded current word
- Create $X, Y$ by looping over corpus, one-hot encoding and storing every bigram encountered

#### How do we find W?

We find $W$ by doing **gradient descent** on the cost:
$$
J=-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^V y_{n,k}\log{p(y_{n,k}|x_n)}
$$
while $J$ not converged:
$$
W \leftarrow W-\eta \triangledown J, \text{ where } \triangledown J=X^T(p(Y|X)-Y)
$$


- If we take the softmax of W, we get $p(current|last)$
- We can also do the opposite operation: $log(bigram_probs)$
  - Softmax() is the opposite of log()



# Neural Network Bigram Model

OLD:
$$
p(y|x)=softmax(W^Tx)
$$
NEW:
$$
h=tanh(W_1^Tx) \\
p(y|x)=softmax(W_2^Th)
$$
<img src="2.Language Models and Neural Network_1.png" style="zoom:50%;" />

- Shape($W_1$)  ==  V x D

- Shape($W_2$)  == D x  V 

  - D<<V

    

## Learning

Cost is exactly the same
$$
\begin{align}
J&=-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^V y_{n,k}\log{p(y_{n,k}|x_n)} \\
\text{while J not converged} \\
W_2 &\leftarrow W_2-\eta H^T(p(Y|X)-Y) \\
W_1 &\leftarrow W_1-\eta X^T[(p(Y|X)-Y)W_2^T \cdot(1-H^2) ]\\
\end{align}
$$


## Logistic Regression v.s. Neural Network

- LR was very slow
- Suppose V=10,000, Then W contains VxV=$10^8$ numbers(100 million)
- Suppose D=100, Then W1 and W2 contains $10^210^4=10^6$ numbers each, 2 million total
- We get a 50:1 compression ratio using a neural network



# Improving Efficiency 







# Review

- What we have been doing  is function approximation for $p(w_t|w_{t-1})$
  $$
  \begin{align}
  \hat{p}(y|x) &= \frac{\text{count}(x \rightarrow y)}{\text{count}(x)} \\
  \hat{p}(y|x) &= softmax(W^Tx) \\
  \hat{p}(y|x) &= softmax(W^T_2tanh(W_1^Tx))
  \end{align}
  $$
  



