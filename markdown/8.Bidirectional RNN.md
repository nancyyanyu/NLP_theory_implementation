# Motivation

- E.x. "General" is as something else in each sentence:
  - "General relativity is an exciting theory about the physics of space and time."
  - "General Zod is an enemy of Superman."
  - "General Motors manufactures cars and trucks."

- An LSTM or GRU looking at the sentence won't see any words before it sees "General", hence must make a prediction based on that word *alone*.



# Basic Ideas

1. We do a standard RNN calculation to get the hidden stats as usual

<img src="8. Bidirectional RNN1.png" style="zoom:40%;" />

2. Make another RNN unit but read the sequence in reverse

<img src="8. Bidirectional RNN2.png" style="zoom:40%;" />

3. Concatinate $h$

<img src="8. Bidirectional RNN3.png" style="zoom:30%;" />



## Many-to-one

- Call the "output" if a Bidirectional RNN:

$$
out=\left[ \overrightarrow{h}_T,\overleftarrow{h}_1 \right]
$$

- In this case both the forward RNN and backward RNN have seen all the words in the sentence.
  - From this we can pass it into a Dense layer calculate $Y$ and so on.
- This is default behavior in Keras if `return_sequences=False`



## When not to use a Bidirectional RNN

- Should not be used when your task is to predict the future, e.x. stock price or weather
- Because generally we get to see the entire input sequence all at once.



# Image Classification with Bidirectional RNNs

















